{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.holoviz.org/panel/1.4.2/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='49f5a38e-5f70-4076-b31c-50163f7124f5'>\n",
       "  <div id=\"c348237f-a277-4417-881d-13cd16b1b444\" data-root-id=\"49f5a38e-5f70-4076-b31c-50163f7124f5\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"13fa0050-76ad-4cdb-99a9-b816ac43750a\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"49f5a38e-5f70-4076-b31c-50163f7124f5\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"0e7e10e3-1da7-43c4-a24a-fee2f327057d\",\"attributes\":{\"plot_id\":\"49f5a38e-5f70-4076-b31c-50163f7124f5\",\"comm_id\":\"c454f338ef5d41019bab73c2fe843fbc\",\"client_comm_id\":\"5356cb0c4dc941dea6fa8018e5bc5090\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"myonmousedown1\",\"properties\":[{\"name\":\"txt12_w\",\"kind\":\"Any\",\"default\":7},{\"name\":\"txt12short_w\",\"kind\":\"Any\",\"default\":7},{\"name\":\"txt14_w\",\"kind\":\"Any\",\"default\":7},{\"name\":\"txt16_w\",\"kind\":\"Any\",\"default\":7},{\"name\":\"txt24_w\",\"kind\":\"Any\",\"default\":7},{\"name\":\"txt36_w\",\"kind\":\"Any\",\"default\":7},{\"name\":\"txt36short_w\",\"kind\":\"Any\",\"default\":7},{\"name\":\"txt48_w\",\"kind\":\"Any\",\"default\":7}]},{\"type\":\"model\",\"name\":\"myonmousewheel1\",\"properties\":[{\"name\":\"mod_inner\",\"kind\":\"Any\",\"default\":\"\\n<svg fill=\\\"#000000\\\" version=\\\"1.1\\\" id=\\\"Capa_1\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\" xmlns:xlink=\\\"http://www.w3.org/1999/xlink\\\" \\n\\t width=\\\"800px\\\" height=\\\"800px\\\" viewBox=\\\"0 0 800 800\\\" xml:space=\\\"preserve\\\">\\n  <rect x=\\\"0\\\" y=\\\"0\\\" width=\\\"800\\\" height=\\\"800\\\" fill=\\\"#ffffff\\\"/> <g> <g>\\n\\t\\t<path d=\\\"M25.555,11.909c-1.216,0-2.207,1.963-2.207,4.396c0,2.423,0.991,4.395,2.207,4.395c1.208,0,2.197-1.972,2.197-4.395\\n\\t\\t\\tC27.751,13.872,26.762,11.909,25.555,11.909z\\\"/>\\n\\t\\t<path d=\\\"M18.22,5.842c4.432,0,6.227,0.335,6.227,3.653h2.207c0-5.851-4.875-5.851-8.433-5.851c-4.422,0-6.227-0.326-6.227-3.644\\n\\t\\t\\tH9.795C9.795,5.842,14.671,5.842,18.22,5.842z\\\"/>\\n\\t\\t<path d=\\\"M29.62,9.495c0.209,0.632,0.331,1.315,0.331,2.031v9.548c0,2.681-1.562,4.91-3.608,5.387\\n\\t\\t\\tc0.004,0.031,0.021,0.059,0.021,0.1v7.67c0,0.445-0.363,0.81-0.817,0.81c-0.445,0-0.809-0.365-0.809-0.81v-7.67\\n\\t\\t\\tc0-0.041,0.019-0.068,0.022-0.1c-2.046-0.477-3.609-2.706-3.609-5.387v-9.548c0-0.715,0.121-1.399,0.331-2.031\\n\\t\\t\\tc-6.057,1.596-10.586,7.089-10.586,13.632v12.716c-0.001,7.787,6.37,14.158,14.155,14.158h0.999\\n\\t\\t\\tc7.786,0,14.156-6.371,14.156-14.158V23.127C40.206,16.584,35.676,11.091,29.62,9.495z\\\"/>\\n\\t</g> </g> </svg>\\n    \"},{\"name\":\"x0_middle\",\"kind\":\"Any\",\"default\":0},{\"name\":\"y0_middle\",\"kind\":\"Any\",\"default\":0},{\"name\":\"x1_middle\",\"kind\":\"Any\",\"default\":0},{\"name\":\"y1_middle\",\"kind\":\"Any\",\"default\":0},{\"name\":\"middle_op_finished\",\"kind\":\"Any\",\"default\":false},{\"name\":\"wheel_x\",\"kind\":\"Any\",\"default\":0},{\"name\":\"wheel_y\",\"kind\":\"Any\",\"default\":0},{\"name\":\"wheel_rots\",\"kind\":\"Any\",\"default\":0},{\"name\":\"wheel_op_finished\",\"kind\":\"Any\",\"default\":false},{\"name\":\"drag_op_finished\",\"kind\":\"Any\",\"default\":false},{\"name\":\"drag_x0\",\"kind\":\"Any\",\"default\":0},{\"name\":\"drag_y0\",\"kind\":\"Any\",\"default\":0},{\"name\":\"drag_x1\",\"kind\":\"Any\",\"default\":10},{\"name\":\"drag_y1\",\"kind\":\"Any\",\"default\":10},{\"name\":\"drag_shiftkey\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"myonmouseup1\",\"properties\":[{\"name\":\"export_string\",\"kind\":\"Any\",\"default\":\"None\"}]}]}};\n",
       "  var render_items = [{\"docid\":\"13fa0050-76ad-4cdb-99a9-b816ac43750a\",\"roots\":{\"49f5a38e-5f70-4076-b31c-50163f7124f5\":\"c348237f-a277-4417-881d-13cd16b1b444\"},\"root_ids\":[\"49f5a38e-5f70-4076-b31c-50163f7124f5\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "49f5a38e-5f70-4076-b31c-50163f7124f5"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, '../../framework')\n",
    "from racetrack import *\n",
    "rt = RACETrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from jsonpath_ng import jsonpath, parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scanForward() - finds the next unescaped version of character c in x starting at i\n",
    "def scanForward(x, i, c):\n",
    "    in_escape = False\n",
    "    while i < len(x):\n",
    "        if   x[i] == '\\\\' and in_escape == False: in_escape = True\n",
    "        else:\n",
    "            if x[i] == c and in_escape == False: return i\n",
    "            in_escape = False\n",
    "        i += 1\n",
    "    return None\n",
    "\n",
    "# literalize() - converts any single or double quoted strings into unique literal names\n",
    "# ... fails if inputs contain four underscore names that overlap with the format \"____lit{num}____\"\n",
    "def literalize(x):\n",
    "    l, lu = [], {}\n",
    "    i = 0\n",
    "    while i < len(x):\n",
    "        c = x[i]\n",
    "        if   c == \"'\":\n",
    "            j = scanForward(x, i+1, \"'\")\n",
    "            if j is None: raise Exception(f'RTOntology.literalize() - unterminated string literal \"{x}\"')\n",
    "            _literal_name_ = f'____lit{len(lu.keys())}____' # Surely, no one would ever use four underscores in a literal... and don't call me Surely\n",
    "            lu[_literal_name_] = x[i+1:j]\n",
    "            l.append(_literal_name_)\n",
    "            i = j + 1\n",
    "        elif c == '\"':\n",
    "            j = scanForward(x, i+1, '\"')\n",
    "            if j is None: raise Exception(f'RTOntology.literalize() - unterminated string literal \"{x}\"')\n",
    "            _literal_name_ = f'____lit{len(lu.keys())}____' # Surely, no one would ever use four underscores in a literal... and don't call me Surely\n",
    "            lu[_literal_name_] = x[i+1:j]\n",
    "            l.append(_literal_name_)\n",
    "            i = j + 1\n",
    "        else:\n",
    "            l.append(c)\n",
    "            i += 1\n",
    "    return ''.join(l), lu\n",
    "\n",
    "# fillLiteratals() - fill in the literal values (opposite of literalize but not guaranteed to keep spaces)\n",
    "def fillLiterals(x, lu):\n",
    "    for k, v in lu.items():\n",
    "        x = x.replace(k, v)\n",
    "    return x\n",
    "\n",
    "# findClosingParen() - find the next closing paren taking other open/closes into consideration\n",
    "# ... requires that literals were taken care of... [see literalize function]\n",
    "def findClosingParen(s, i):\n",
    "    stack = 0\n",
    "    while i < len(s):\n",
    "        if   s[i] == '(':               stack += 1\n",
    "        elif s[i] == ')' and stack > 0: stack -= 1\n",
    "        elif s[i] == ')':               return i\n",
    "        i += 1\n",
    "    raise Exception(f'RTOntology.findClosingParen() - no closing paren found for \"{s}\"')\n",
    "\n",
    "# tokenizeParameters() - create a token list of function parameters\n",
    "# ... requires that literals were taken care of... [see literalize function]\n",
    "def tokenizeParameters(x):\n",
    "    r = []\n",
    "    while ',' in x or '(' in x:\n",
    "        if   ',' in x and '(' in x: # both... process the one that occurs first\n",
    "            i = x.index(',')\n",
    "            j = x.index('(')\n",
    "            if i < j:\n",
    "                r.append(x[:i])\n",
    "                x = x[i+1:]\n",
    "            else:\n",
    "                k = findClosingParen(x,j+1)\n",
    "                r.append(x[:k+1].strip())\n",
    "                x = x[k+1:]\n",
    "                if ',' in x: x = x[x.index(',')+1:] # if there's another comma, consume it\n",
    "        elif ',' in x: # just literals from here on out...\n",
    "            r.append(x[:x.index(',')].strip())\n",
    "            x = x[x.index(',')+1:]\n",
    "        elif '(' in x: # just one function call from here on out...\n",
    "            i = x.index('(')\n",
    "            j = findClosingParen(x,i+1)\n",
    "            r.append(x[:j+1].strip())\n",
    "            x = x[j+1:]\n",
    "            if ',' in x: x = x[x.index(',')+1:] # if there's another comma, consume it\n",
    "    x = x.strip()\n",
    "    if len(x) > 0:\n",
    "        r.append(x)\n",
    "    return r\n",
    "\n",
    "# parseTree() - create a parse tree representation of a ontology node description\n",
    "def parseTree(x, node_value=None, node_children=None, node_name=None, lit_lu=None):\n",
    "    if node_value is None:\n",
    "        node_value = {}\n",
    "        node_children = {}\n",
    "        node_name = 'root'\n",
    "\n",
    "    if lit_lu is None:\n",
    "        x, lit_lu = literalize(x)\n",
    "    if '(' in x:\n",
    "        i          = x.index('(')\n",
    "        j          = findClosingParen(x, i+1)\n",
    "        fname      = x[0:i].strip()\n",
    "        parms      = tokenizeParameters(x[i+1:j])\n",
    "        node_value   [node_name] = lit_lu[fname] if fname in lit_lu else fname\n",
    "        node_children[node_name] = []    # functions have children... even if it's an empty list of children\n",
    "        for child_i in range(len(parms)):\n",
    "            child_name = f'{node_name}.{child_i}'\n",
    "            node_children[node_name].append(child_name)\n",
    "            parseTree(parms[child_i], node_value, node_children, child_name, lit_lu)\n",
    "    else:\n",
    "        x                        = x.strip()\n",
    "        node_value   [node_name] = lit_lu[x] if x in lit_lu else x\n",
    "        node_children[node_name] = None # literals have no children\n",
    "    return node_value, node_children\n",
    "\n",
    "# solveParseTree() - evaluate a parse tree\n",
    "def solveParseTree(values, children, filled, i, node=None):\n",
    "    if node is None: node = 'root'\n",
    "    if   children[node] is None and isJsonPath(values[node]):\n",
    "        return filled[values[node]][i]  # jsonpath filled in value from the json\n",
    "    elif children[node] is None:\n",
    "        return values[node]             # constant / literal\n",
    "    else:\n",
    "        parms = [solveParseTree(values, children, filled, i, x) for x in children[node]]\n",
    "        return eval(f'{values[node]}(*parms)')\n",
    "\n",
    "# upToStar() - upto the cth '[*]'\n",
    "def upToStar(x, c):\n",
    "    i = 0\n",
    "    while c > 0:\n",
    "        j = x.index('[*]', i)\n",
    "        i = j + 3\n",
    "        c -= 1\n",
    "    return x[:i]\n",
    "\n",
    "# fillStars() - fill the the stars in the specified order\n",
    "def fillStars(x, i, j=None, k=None):\n",
    "    if '[*]' not in x: return x # for example ... \"$.id\"\n",
    "    _index_ = x.index('[*]')\n",
    "    x = x[:_index_] + f'[{i}]' + x[_index_+3:]\n",
    "    if j is not None and '[*]' in x:\n",
    "        _index_ = x.index('[*]')\n",
    "        x = x[:_index_] + f'[{j}]' + x[_index_+3:]\n",
    "    if k is not None and '[*]' in x:\n",
    "        _index_ = x.index('[*]')\n",
    "        x = x[:_index_] + f'[{k}]' + x[_index_+3:]\n",
    "    return x\n",
    "\n",
    "# isJsonPath() - check if the string is a jsonpath\n",
    "def isJsonPath(_str_): \n",
    "    return _str_.startswith('$.') or _str_.startswith('$[')\n",
    "\n",
    "#\n",
    "# fillJSONPathElementsByJSONPath() - unoptimized version using jsonpath-ng\n",
    "# - to_fill should only have values with a [*] in them\n",
    "#\n",
    "def fillJSONPathElementsByJSONPath(to_fill, myjson):\n",
    "    longest_by_star_path, min_stars, max_stars = None, None, None\n",
    "    filled = {}\n",
    "    for x in to_fill:\n",
    "        star_count = x.count('[*]')\n",
    "        if min_stars is None or star_count < min_stars: min_stars = star_count\n",
    "        if max_stars is None or star_count > max_stars: max_stars = star_count\n",
    "        if longest_by_star_path is None or len(x) > len(longest_by_star_path): longest_by_star_path = x\n",
    "        filled[x] = []\n",
    "\n",
    "    # fill in the json values into the filled dict\n",
    "    if min_stars == max_stars: # shortcut if we only do the same number of stars\n",
    "        _length_to_fill_ = None\n",
    "        for v in filled.keys():\n",
    "            if isJsonPath(v) and '[*]' in v:\n",
    "                filled[v] = [match.value if match.value != {} else None for match in parse(v).find_or_create(myjson)]\n",
    "                _this_length_ = len(filled[v])\n",
    "                if   _length_to_fill_ is None:          _length_to_fill_ = _this_length_\n",
    "                elif _length_to_fill_ != _this_length_: raise Exception(f'RTOntology.__applyTemplate__() - unequal number of values for {v} ({_length_to_fill_=} vs {_this_length_=})')\n",
    "        for v in filled.keys():\n",
    "            if   isJsonPath(v) and '[*]' not in v:\n",
    "                _match_ = parse(v).find(myjson)[0].value\n",
    "                filled[v] = [_match_] * _length_to_fill_\n",
    "            elif isJsonPath(v) == False:\n",
    "                filled[v] = [v] * _length_to_fill_\n",
    "    else:\n",
    "        star_count = longest_by_star_path.count('[*]')\n",
    "        if star_count   == 1:\n",
    "            for i in range(len(parse(upToStar(longest_by_star_path, 1)).find(myjson))):\n",
    "                for v in filled.keys():\n",
    "                    if isJsonPath(v):\n",
    "                        _matches_ = parse(fillStars(v, i)).find(myjson)\n",
    "                        if len(_matches_) == 1: filled[v].append(_matches_[0].value)\n",
    "                        else:                   filled[v].append(None)\n",
    "                    else:\n",
    "                        filled[v].append(v)\n",
    "        elif star_count == 2:\n",
    "            for i in range(len(parse(upToStar(longest_by_star_path, 1)).find(myjson))):\n",
    "                for j in range(len(parse(upToStar(fillStars(longest_by_star_path,i), 1)).find(myjson))):\n",
    "                    for v in filled.keys():\n",
    "                        if isJsonPath(v):\n",
    "                            _matches_ = parse(fillStars(v, i, j)).find(myjson)\n",
    "                            if len(_matches_) == 1: filled[v].append(_matches_[0].value)\n",
    "                            else:                   filled[v].append(None)\n",
    "                        else:\n",
    "                            filled[v].append(v)\n",
    "        elif star_count == 3:\n",
    "            for i in range(len(parse(upToStar(longest_by_star_path, 1)).find(myjson))):\n",
    "                for j in range(len(parse(upToStar(fillStars(longest_by_star_path,i), 1)).find(myjson))):\n",
    "                    for k in range(len(parse(upToStar(fillStars(longest_by_star_path,i,j), 1)).find(myjson))):\n",
    "                        for v in filled.keys():\n",
    "                            if isJsonPath(v):\n",
    "                                _matches_ = parse(fillStars(v, i, j, k)).find(myjson)\n",
    "                                if len(_matches_) == 1: filled[v].append(_matches_[0].value)\n",
    "                                else:                   filled[v].append(None)\n",
    "                            else:\n",
    "                                filled[v].append(v)\n",
    "        else:\n",
    "            raise Exception(f'RTOntology.__applyTemplate__() - max of three stars supported -- {star_count} found')\n",
    "    return filled\n",
    "\n",
    "#\n",
    "# isLiteral() - true if it's a proper literal for json key string\n",
    "#\n",
    "def isLiteral(v):\n",
    "    if v == '': return False\n",
    "    for i in range(len(v)):\n",
    "        if i == 0 and v[i] >= '0' and v[i] <= '9': return False # can't start with a number\n",
    "        if (v[i] >= 'a' and v[i] <= 'z') or (v[i] >= 'A' and v[i] <= 'Z') or (v[i] >= '0' and v[i] <= '9') or (v[i] == '_'): pass\n",
    "        else: return False\n",
    "    return True\n",
    "\n",
    "#\n",
    "# endsWithAny() - does a string end with any of these?\n",
    "#\n",
    "def endsWithAny(_str_, _set_):\n",
    "    return any(_str_.endswith(x) for x in _set_)\n",
    "\n",
    "#\n",
    "# fillJSONPathElements() - uses self modifying code to optimize the filling of the structures based on jsonpath specifications.\n",
    "#\n",
    "def fillJSONPathElements(to_fill, myjson):\n",
    "    filled = {}\n",
    "    for x in to_fill: filled[x] = [] \n",
    "    filled_list = list(filled.keys())\n",
    "    longest_by_star_path  = filled_list[0]\n",
    "    for i in range(1, len(filled_list)): \n",
    "        if longest_by_star_path.count('[*]') < filled_list[i].count('[*]'): longest_by_star_path = filled_list[i]\n",
    "    to_eval, indent, _index_, _loop_vars_, _loop_i_, _path_, _star_path_, vars_set = [], 0, 1, ['i','j','k','l'], 0, '', '$', 0\n",
    "    while _index_ < len(longest_by_star_path):\n",
    "        _rest_ = longest_by_star_path[_index_:]\n",
    "        if   _rest_.startswith('[*]'):\n",
    "            to_eval.append(' '*indent+'for '+_loop_vars_[_loop_i_]+f' in range(len(myjson{_path_})):')\n",
    "            _path_      += f'[{_loop_vars_[_loop_i_]}]'\n",
    "            _star_path_ += f'[*]'\n",
    "            _index_, _loop_i_, indent = _index_+3, _loop_i_+1, indent+4\n",
    "            if _rest_.endswith('[*]'):\n",
    "                for i in range(len(filled_list)):\n",
    "                    if filled_list[i] == _star_path_:\n",
    "                        to_eval.append(' '*indent+f'_var{i}_ = myjson{_path_}')\n",
    "                        vars_set += 1\n",
    "            for i in range(len(filled_list)):\n",
    "                _filled_rest_ = filled_list[i][_index_:]\n",
    "                if _filled_rest_.count('[*]') == 0 and '.' not in _filled_rest_ and len(_filled_rest_) > 0:\n",
    "                    to_eval.append(' '*indent+f'_var{i}_ = myjson{_path_}' + _filled_rest_)\n",
    "                    vars_set += 1\n",
    "\n",
    "        elif _rest_[0] == '.':\n",
    "            _star_path_ += '.'\n",
    "            for i in range(len(filled_list)):\n",
    "                lit_maybe = filled_list[i][len(_star_path_):]\n",
    "                if isLiteral(lit_maybe):\n",
    "                    to_eval.append(' '*indent+f'if \"{lit_maybe}\" in myjson{_path_}:')\n",
    "                    to_eval.append(' '*(indent+4)+f'_var{i}_ = myjson{_path_}[\"{lit_maybe}\"]')\n",
    "                    to_eval.append(' '*indent+f'else: _var{i}_ = None')\n",
    "                    vars_set += 1\n",
    "            _index_ += 1\n",
    "        elif _rest_[0].isalpha() or _rest_[0] == '_':\n",
    "            l = len(_rest_)\n",
    "            if '.' in _rest_:                           l = _rest_.index('.')\n",
    "            if '[' in _rest_ and _rest_.index('[') < l: l = _rest_.index('[')\n",
    "            lit = _rest_[:l]\n",
    "            to_eval.append(' '*indent+f'if \"{lit}\" in myjson{_path_}:')\n",
    "            _path_      += f'[\"{lit}\"]'\n",
    "            _star_path_ += f'{lit}'\n",
    "            _index_, indent = _index_+l, indent+4\n",
    "        else:\n",
    "            print('Exception for the following script:\\n')\n",
    "            print('\\n'.join(to_eval)) \n",
    "            raise Exception(f'RTOntology.fillJSONPathElements() - parse error at {i}')\n",
    "\n",
    "        if vars_set >= len(filled_list):\n",
    "            for i in range(len(filled_list)):\n",
    "                to_eval.append(' '*indent+f'filled[\"{filled_list[i]}\"].append(_var{i}_)')\n",
    "            break\n",
    "\n",
    "    if to_eval[-1].endswith(':'): to_eval.append(' '*indent+'pass')\n",
    "    # print('\\n'.join(to_eval))\n",
    "    exec('\\n'.join(to_eval))\n",
    "    return filled\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class RTOntology(object):\n",
    "    # __init__() - prepare transform spec for use and initial instance variables\n",
    "    def __init__(self, xform_spec=None):\n",
    "        if xform_spec is not None: self.xform_spec_lines = self.__substituteDefines__(xform_spec)\n",
    "        else:                      self.xform_spec_lines = []\n",
    "        self.df_triples = None\n",
    "        self.uid_lu     = {}\n",
    "        self.rev_uid_lu = {}\n",
    "        self.time_lu    = {}\n",
    "        for x in ['fill.trace_json_paths', 'fill.collapse', 'fill.parse']: self.time_lu[x] = 0\n",
    "\n",
    "    # to_files() - write state to several files\n",
    "    def to_files(self, _base_name_):\n",
    "        self.df_triples.write_parquet(f'{_base_name_}.triples.parquet')\n",
    "        _lu_ = {'uid':[], 't0':[], 't1':[], 't2':[]}\n",
    "        for _uid_ in self.uid_lu:\n",
    "            _lu_['uid'].append(_uid_)\n",
    "            _lu_['t0'].append(self.uid_lu[_uid_][0])\n",
    "            _lu_['t1'].append(self.uid_lu[_uid_][1])\n",
    "            _lu_['t2'].append(self.uid_lu[_uid_][2])\n",
    "        pd.DataFrame(_lu_).to_parquet(f'{_base_name_}.uids.parquet')\n",
    "        if len(self.xform_spec_lines) > 0:\n",
    "            with open(f'{_base_name_}.xform_spec', 'wt') as f: f.write('\\n'.join(self.xform_spec_lines))\n",
    "\n",
    "    # fm_files() - read state from several files\n",
    "    def fm_files(self, _base_name_):\n",
    "        self.df_triples = pd.read_parquet(f'{_base_name_}.triples.parquet')\n",
    "        _lu_ = pd.read_parquet(f'{_base_name_}.uids.parquet')\n",
    "        uid_v, t0_v, t1_v, t2_v = _lu_['uid'].values, _lu_['t0'].values, _lu_['t1'].values, _lu_['t2'].values\n",
    "        for i in range(len(uid_v)):\n",
    "            self.uid_lu[uid_v[i]] = (t0_v[i], t1_v[i], t2_v[i])\n",
    "            if t2_v[i] == 'uniq':\n",
    "                _key_ = str(t0_v[i]) + '|' + str(t1_v[i])\n",
    "                self.rev_uid_lu[_key_] = uid_v[i]\n",
    "                \n",
    "    # __substituteDefines__() - subsitute defines\n",
    "    def __substituteDefines__(self, _txt_):\n",
    "        lines     = _txt_.split('\\n')\n",
    "        subs      = {}\n",
    "        completes = []\n",
    "        for _line_ in lines:\n",
    "            tokens = _line_.split()\n",
    "            if len(tokens) >= 3 and tokens[1] == '=':\n",
    "                subs[tokens[0]] = ' '.join(tokens[2:])\n",
    "            else:\n",
    "                for r in subs:\n",
    "                    if r in _line_:\n",
    "                        _line_ = _line_.replace(r, subs[r])\n",
    "                if len(_line_) > 0:\n",
    "                    completes.append(_line_)\n",
    "        return completes\n",
    "\n",
    "    # __applyTemplate__() - apply templated line in the transform to the json representation\n",
    "    def __applyTemplate__(self, \n",
    "                          myjson,        # json representation\n",
    "                          s_values,      s_children,    s_type,     s_disp, # subject params\n",
    "                          v_values,      v_children,                        # verb params   (it's only a string, no typing, unique to the schema)\n",
    "                          o_values,      o_children,    o_type,     o_disp, # object params\n",
    "                          g_values,      g_children,    g_type,     g_disp, # group params\n",
    "                          src_values,    src_children,                      # source params (it's only a string, no typing, unique to this ontological instance)\n",
    "                          ):\n",
    "        # resolve the jsonpath values        \n",
    "        all_values  = set(s_values.values()) | set(v_values.values()) | set(o_values.values())\n",
    "        if g_values   is not None: all_values |= set(g_values.values())\n",
    "        if src_values is not None: all_values |= set(src_values.values())\n",
    "\n",
    "        t0 = time.time()\n",
    "        path_values, starred_path_values, longest_by_star_path = [], [], None\n",
    "        for x in all_values:\n",
    "            if isJsonPath(x):\n",
    "                if '[*]' in x:\n",
    "                    starred_path_values.append(x)\n",
    "                    if   longest_by_star_path is None:                          longest_by_star_path = x\n",
    "                    elif longest_by_star_path.rindex('[*]') < x.rindex('[*]'):  longest_by_star_path = x\n",
    "                else:\n",
    "                    path_values.append(x)\n",
    "\n",
    "        # ensure that all jsonpath values are substrings of the longest star path\n",
    "        for x in starred_path_values:\n",
    "            x_until_last_star = x[:x.rindex('[*]')+3] # get the close bracket too\n",
    "            if longest_by_star_path[:len(x_until_last_star)] != x_until_last_star:\n",
    "                raise Exception(f'OntologyForViz.__applyTemplate__() - jsonpath are not subsets \"{x}\" vs \"{longest_by_star_path}\"')\n",
    "\n",
    "        # fill in the json values into the filled dict\n",
    "        if len(starred_path_values) > 0: filled = fillJSONPathElements(starred_path_values, myjson)\n",
    "        else:                            filled = {}\n",
    "\n",
    "        # ... double check that they are the same length\n",
    "        fill_len = None\n",
    "        for x in filled.keys():\n",
    "            if fill_len is None: fill_len = len(filled[x])\n",
    "            if len(filled[x]) != fill_len: raise Exception(f'OntologyForViz.__applyTemplate__() - unequal number of values for {x}')\n",
    "        if fill_len is None: fill_len = 1 # if there are no starred paths, then we need at least one filler (it's a constant path)\n",
    "\n",
    "        # Fix up the filled with either constants or with static json paths\n",
    "        for v in all_values:\n",
    "            if isJsonPath(v) and '[*]' in v: continue\n",
    "            if    isJsonPath(v): to_fill = [parse(v).find(myjson)[0].value]\n",
    "            else:                to_fill = [v]\n",
    "            filled[v] = to_fill * fill_len\n",
    "        t1 = time.time()\n",
    "        self.time_lu['fill.trace_json_paths'] += (t1-t0)\n",
    "\n",
    "        # collapse the parse trees based on the filled values\n",
    "        # ... double check that they are the same length\n",
    "        t0 = time.time()\n",
    "        l = None\n",
    "        for v in filled.keys():\n",
    "            if l is None: l = len(filled[v])\n",
    "            if len(filled[v]) != l: raise Exception(f'RTOntology.__applyTemplate__() - unequal number of values for {v}')\n",
    "        pre_df = {}\n",
    "        pre_df['sbj']    = [solveParseTree(s_values,   s_children,   filled, i) for i in range(l)]\n",
    "        pre_df['vrb']    = [solveParseTree(v_values,   v_children,   filled, i) for i in range(l)]\n",
    "        pre_df['obj']    = [solveParseTree(o_values,   o_children,   filled, i) for i in range(l)]\n",
    "        if g_values   is not None: pre_df['grp'] = [solveParseTree(g_values,   g_children,   filled, i) for i in range(l)]\n",
    "        if src_values is not None: pre_df['src'] = [solveParseTree(src_values, src_children, filled, i) for i in range(l)]\n",
    "        t1 = time.time()\n",
    "        self.time_lu['fill.collapse'] += (t1-t0)\n",
    "\n",
    "        t0 = time.time()\n",
    "        for_df = {'sbj': [], 'stype': [], 'sdisp': [], 'vrb': [], 'obj': [], 'otype': [], 'odisp': [], 'grp':[], 'gdisp':[], 'src':[]}\n",
    "        for i in range(l):\n",
    "            #\n",
    "            # Subject (Required)\n",
    "            #\n",
    "            _sbj_, _sbj_type_, _sbj_disp_ = pre_df['sbj'][i], s_type, s_disp\n",
    "            if type(_sbj_) == tuple:\n",
    "                _sbj_type_ = _sbj_[1] if len(_sbj_) > 1 else s_type\n",
    "                _sbj_disp_ = _sbj_[2] if len(_sbj_) > 2 else s_disp\n",
    "                _sbj_      = _sbj_[0]\n",
    "            _sbj_uid_ = self.resolveUniqIdAndUpdateLookups(_sbj_, _sbj_type_, _sbj_disp_, 'sbj')\n",
    "            for_df['sbj'].append(_sbj_uid_), for_df['stype'].append(_sbj_type_), for_df['sdisp'].append(_sbj_disp_)\n",
    "\n",
    "            #\n",
    "            # Verb (Required)\n",
    "            #\n",
    "            _vrb_ = pre_df['vrb'][i]\n",
    "            for_df['vrb'].append(_vrb_)\n",
    "\n",
    "            #\n",
    "            # Object (Required)\n",
    "            #\n",
    "            _obj_, _obj_type_, _obj_disp_ = pre_df['obj'][i], o_type, o_disp\n",
    "            if type(_obj_) == tuple:\n",
    "                _obj_type_ = _obj_[1] if len(_obj_) > 1 else o_type\n",
    "                _obj_disp_ = _obj_[2] if len(_obj_) > 2 else o_disp\n",
    "                _obj_      = _obj_[0]\n",
    "            _obj_uid_ = self.resolveUniqIdAndUpdateLookups(_obj_, _obj_type_, _obj_disp_, 'obj')            \n",
    "            for_df['obj'].append(_obj_uid_), for_df['otype'].append(_obj_type_), for_df['odisp'].append(_obj_disp_)\n",
    "\n",
    "            #\n",
    "            # Grouping (Optional)\n",
    "            #\n",
    "            if g_values is not None:\n",
    "                _grp_, _grp_type_, _grp_disp_ = pre_df['grp'][i], g_type, g_disp\n",
    "                if type(_grp_) == tuple:\n",
    "                    _grp_type_ = _grp_[1] if len(_grp_) > 1 else g_type\n",
    "                    _grp_disp_ = _grp_[2] if len(_grp_) > 2 else g_disp\n",
    "                    _grp_      = _grp_[0]\n",
    "                _grp_uid_ = self.resolveUniqIdAndUpdateLookups(_grp_, _grp_type_, _grp_disp_, 'grp')\n",
    "                for_df['grp'].append(_grp_uid_)\n",
    "                for_df['gdisp'].append(_grp_disp_)\n",
    "            else:\n",
    "                for_df['grp'].append(None)\n",
    "                for_df['gdisp'].append(None)\n",
    "\n",
    "            #\n",
    "            # Sourcing (Optional)\n",
    "            #\n",
    "            if src_values is not None:\n",
    "                _src_ = pre_df['src'][i]\n",
    "                for_df['src'].append(str(_src_))\n",
    "            else:\n",
    "                for_df['src'].append(None)\n",
    "\n",
    "        t1 = time.time()\n",
    "        self.time_lu['fill.parse'] += (t1-t0)\n",
    "\n",
    "        _df_            = pl.DataFrame(for_df)\n",
    "        if len(_df_) > 0: self.df_triples = _df_ if self.df_triples is None else pl.concat([_df_, self.df_triples])\n",
    "\n",
    "    # resolveIdAndUpdateLookups() - resolve id and update lookups\n",
    "    # self.uid_lu[<interger>] = (id-from-input, type-from-input, disposition-from-input)\n",
    "    #\n",
    "    def resolveUniqIdAndUpdateLookups(self, _id_, _type_, _disp_, _occurs_in_):\n",
    "        _uniq_key_ = str(_id_)+'|'+str(_type_)\n",
    "        if _disp_ == 'uniq' and _uniq_key_ in self.rev_uid_lu: return self.rev_uid_lu[_uniq_key_]\n",
    "        my_uid = 100_000 + len(self.uid_lu.keys())\n",
    "        self.uid_lu[my_uid] = (_id_, _type_, _disp_)\n",
    "        if _disp_ == 'uniq':  self.rev_uid_lu[_uniq_key_] = my_uid\n",
    "        return my_uid\n",
    "\n",
    "    # parse() - parse json into ontology via specification\n",
    "    def parse(self, j):\n",
    "        for l in self.xform_spec_lines:\n",
    "            l, lu = literalize(l) # get rid of any literal values so it doesn't mess up the delimiters\n",
    "            if '#' in l: l = l[:l.index('#')].strip() # comments... hope the hash symbol doesn't occur anywhere in the template that isn't a comment\n",
    "            if len(l) == 0: continue\n",
    "\n",
    "            # Sourcing Information\n",
    "            src_values = src_children = None\n",
    "            if '^^^' in l:\n",
    "                src = l[l.index('^^^')+3:]\n",
    "                l   = l[:l.index('^^^')].strip()\n",
    "                src_values, src_children = parseTree(fillLiterals(src, lu))\n",
    "\n",
    "            # Grouping Information\n",
    "            g_values = g_children = g_type = g_disp = None\n",
    "            if '@@@' in l:\n",
    "                grp = l[l.index('@@@')+3:]\n",
    "                l   = l[:l.index('@@@')].strip()\n",
    "                g_uniq = None\n",
    "                if endsWithAny(grp, {'uniq', 'ambi', 'anon', 'yyyy', 'dura', 'cata', 'valu', 'cont'}) and '|' in grp:\n",
    "                    g_disp = grp[grp.rindex('|')+1:].strip()\n",
    "                    grp    = grp[:grp.rindex('|')]\n",
    "                else: g_disp = 'ambi'\n",
    "                g_type = None\n",
    "                if '|' in grp:\n",
    "                    g_type = grp[grp.rindex('|')+1:].strip()\n",
    "                    grp   = grp[:grp.rindex('|')]\n",
    "                g_node = grp\n",
    "                g_values, g_children = parseTree(fillLiterals(g_node, lu))\n",
    "                \n",
    "            svo = [x.strip() for x in l.split('---')]\n",
    "            if len(svo) == 3:\n",
    "                s, v, o = svo[0], svo[1], svo[2]\n",
    "\n",
    "                # Subject\n",
    "                s_uniq = None\n",
    "                if endsWithAny(s, {'uniq', 'ambi', 'anon', 'yyyy', 'dura', 'cata', 'valu', 'cont'}) and '|' in s:\n",
    "                    s_disp = s[s.rindex('|')+1:].strip()\n",
    "                    s      = s[:s.rindex('|')]\n",
    "                else: s_disp = 'ambi'\n",
    "                s_type = None\n",
    "                if '|' in s:\n",
    "                    s_type = s[s.rindex('|')+1:].strip()\n",
    "                    s      = s[:s.rindex('|')]\n",
    "                s_node = s\n",
    "                s_values, s_children = parseTree(fillLiterals(s_node, lu))\n",
    "\n",
    "                # Verb\n",
    "                v_values, v_children = parseTree(fillLiterals(v, lu))\n",
    "\n",
    "                # Object\n",
    "                o_uniq = None\n",
    "                if endsWithAny(o, {'uniq', 'ambi', 'anon', 'yyyy', 'dura', 'cata', 'valu', 'cont'}) and '|' in o:\n",
    "                    o_disp = o[o.rindex('|')+1:].strip()\n",
    "                    o      = o[:o.rindex('|')]\n",
    "                else: o_disp = 'ambi'\n",
    "                if '|' in o:\n",
    "                    o_type = o[o.rindex('|')+1:].strip()\n",
    "                    o      = o[:o.rindex('|')]\n",
    "                o_node = o\n",
    "                o_values, o_children = parseTree(fillLiterals(o_node, lu))\n",
    "                self.__applyTemplate__(j, s_values, s_children, s_type, s_disp, \n",
    "                                          v_values, v_children, \n",
    "                                          o_values, o_children, o_type, o_disp,\n",
    "                                          g_values, g_children, g_type, g_disp,\n",
    "                                          src_values, src_children)\n",
    "            else:\n",
    "                raise Exception(f'RTOntology.parse() - line \"{l}\" does not have three parts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_json_txt_ = '''\n",
    "{\"id\":1,\n",
    " \"people\":[{\"first\":\"John\", \"last\":\"Smith\", \"id\":10, \"citescore\":2.3, \"age\":30, \"city\":\"nyc\",          \"state\":\"ny\", \"country\":\"us\"},\n",
    "           {\"first\":\"Joe\",  \"last\":\"Smith\", \"id\":20, \"citescore\":1.8, \"age\":35,                        \"state\":\"ny\", \"country\":\"us\"},\n",
    "           {\"first\":\"Mary\", \"last\":\"Jones\", \"id\":30, \"age\":32, \"city\":\"philadelphia\", \"state\":\"pa\", \"country\":\"us\"}],\n",
    " \"knowsFrom\":[[10, 20, \"Conference A\"], \n",
    "              [20, 30, \"Conference B\"]],\n",
    " \"education\":[{\"id\":10, \"degreeReceived\":\"Ph.D. in Computer Science\",   \"university\":\"Stanford University\"},\n",
    "              {\"id\":10, \"degreeReceived\":\"Masters in Computer Science\", \"university\":\"University of Pennsylvania\"}],\n",
    " \"total_people\":3\n",
    "}'''\n",
    "_json_simple_  = json.loads(_json_txt_)\n",
    "def concatNames(_last_,_first_):\n",
    "    return _last_ + ' ' + _first_\n",
    "def combineAddress(_city_,_state_,_country_):\n",
    "    s = ''\n",
    "    if _city_    is not None: s += _city_\n",
    "    if _state_   is not None: s += ', ' + _state_    if (len(s) > 0) else _state_\n",
    "    if _country_ is not None: s += ', ' + _country_  if (len(s) > 0) else _country_\n",
    "    return s if (len(s) > 0) else 'Not Supplied'\n",
    "_xform_simple_ = '''\n",
    "_id_ = '$.people[*].id' | PersonID | uniq\n",
    "'$.id'                                --- \"hasEntryCount\"    --- '$.total_people' | xsd:integer                                                                           ^^^ \"IN_TEMPLATE\"\n",
    "_id_                                  --- \"hasName\"          --- concatNames('$.people[*].last', '$.people[*].first') | xsd:string                                        ^^^ \"IN_TEMPLATE\"\n",
    "_id_                                  --- \"hasCitationScore\" --- '$.people[*].citescore' | xsd:float   | valu                                                             ^^^ '$.id'    \n",
    "_id_                                  --- \"hasAge\"           --- '$.people[*].age'       | xsd:integer | valu                                                             ^^^ '$.id'\n",
    "_id_                                  --- \"isFrom\"           --- combineAddress('$.people[*].city', '$.people[*].state', '$.people[*].country') | CityStateCountry | uniq ^^^ '$.id'\n",
    "_id_                                  --- \"isFromCity\"       --- '$.people[*].city'      | City                                                                           ^^^ '$.id'\n",
    "'$.knowsFrom[*][0]' | PersonID | uniq --- \"knows\"            --- '$.knowsFrom[*][1]'     | PersonID    | uniq                 @@@ '$.knowsFrom[*][2]' | xsd:string | uniq ^^^ '$.id'\n",
    "'''\n",
    "ofv_simple = RTOntology(_xform_simple_)\n",
    "ofv_simple.parse(_json_simple_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ofv_simple.uid_lu={100000: (1, None, 'ambi'), 100001: (3, 'xsd:integer', 'ambi'), 100002: (10, 'PersonID', 'uniq'), 100003: ('Smith John', 'xsd:string', 'ambi'), 100004: (20, 'PersonID', 'uniq'), 100005: ('Smith Joe', 'xsd:string', 'ambi'), 100006: (30, 'PersonID', 'uniq'), 100007: ('Jones Mary', 'xsd:string', 'ambi'), 100008: (2.3, 'xsd:float', 'valu'), 100009: (1.8, 'xsd:float', 'valu'), 100010: (None, 'xsd:float', 'valu'), 100011: (30, 'xsd:integer', 'valu'), 100012: (35, 'xsd:integer', 'valu'), 100013: (32, 'xsd:integer', 'valu'), 100014: ('nyc, ny, us', 'CityStateCountry', 'uniq'), 100015: ('ny, us', 'CityStateCountry', 'uniq'), 100016: ('philadelphia, pa, us', 'CityStateCountry', 'uniq'), 100017: ('nyc', 'City', 'ambi'), 100018: (None, 'City', 'ambi'), 100019: ('philadelphia', 'City', 'ambi'), 100020: ('Conference A', 'xsd:string', 'uniq'), 100021: ('Conference B', 'xsd:string', 'uniq')}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (18, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sbj</th><th>stype</th><th>sdisp</th><th>vrb</th><th>obj</th><th>otype</th><th>odisp</th><th>grp</th><th>gdisp</th><th>src</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>100002</td><td>&quot;PersonID&quot;</td><td>&quot;uniq&quot;</td><td>&quot;knows&quot;</td><td>100004</td><td>&quot;PersonID&quot;</td><td>&quot;uniq&quot;</td><td>100020</td><td>&quot;uniq&quot;</td><td>&quot;1&quot;</td></tr><tr><td>100004</td><td>&quot;PersonID&quot;</td><td>&quot;uniq&quot;</td><td>&quot;knows&quot;</td><td>100006</td><td>&quot;PersonID&quot;</td><td>&quot;uniq&quot;</td><td>100021</td><td>&quot;uniq&quot;</td><td>&quot;1&quot;</td></tr><tr><td>100002</td><td>&quot;PersonID&quot;</td><td>&quot;uniq&quot;</td><td>&quot;isFromCity&quot;</td><td>100017</td><td>&quot;City&quot;</td><td>&quot;ambi&quot;</td><td>null</td><td>null</td><td>&quot;1&quot;</td></tr><tr><td>100004</td><td>&quot;PersonID&quot;</td><td>&quot;uniq&quot;</td><td>&quot;isFromCity&quot;</td><td>100018</td><td>&quot;City&quot;</td><td>&quot;ambi&quot;</td><td>null</td><td>null</td><td>&quot;1&quot;</td></tr><tr><td>100006</td><td>&quot;PersonID&quot;</td><td>&quot;uniq&quot;</td><td>&quot;isFromCity&quot;</td><td>100019</td><td>&quot;City&quot;</td><td>&quot;ambi&quot;</td><td>null</td><td>null</td><td>&quot;1&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>100006</td><td>&quot;PersonID&quot;</td><td>&quot;uniq&quot;</td><td>&quot;hasCitationSco</td><td>100010</td><td>&quot;xsd:float&quot;</td><td>&quot;valu&quot;</td><td>null</td><td>null</td><td>&quot;1&quot;</td></tr><tr><td>100002</td><td>&quot;PersonID&quot;</td><td>&quot;uniq&quot;</td><td>&quot;hasName&quot;</td><td>100003</td><td>&quot;xsd:string&quot;</td><td>&quot;ambi&quot;</td><td>null</td><td>null</td><td>&quot;IN_TEMPLATE&quot;</td></tr><tr><td>100004</td><td>&quot;PersonID&quot;</td><td>&quot;uniq&quot;</td><td>&quot;hasName&quot;</td><td>100005</td><td>&quot;xsd:string&quot;</td><td>&quot;ambi&quot;</td><td>null</td><td>null</td><td>&quot;IN_TEMPLATE&quot;</td></tr><tr><td>100006</td><td>&quot;PersonID&quot;</td><td>&quot;uniq&quot;</td><td>&quot;hasName&quot;</td><td>100007</td><td>&quot;xsd:string&quot;</td><td>&quot;ambi&quot;</td><td>null</td><td>null</td><td>&quot;IN_TEMPLATE&quot;</td></tr><tr><td>100000</td><td>null</td><td>&quot;ambi&quot;</td><td>&quot;hasEntryCount&quot;</td><td>100001</td><td>&quot;xsd:integer&quot;</td><td>&quot;ambi&quot;</td><td>null</td><td>null</td><td>&quot;IN_TEMPLATE&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (18, 10)\n",
       "\n",
       " sbj     stype     sdisp  vrb                 odisp  grp     gdisp  src         \n",
       " ---     ---       ---    ---                  ---    ---     ---    ---         \n",
       " i64     str       str    str                  str    i64     str    str         \n",
       "\n",
       " 100002  PersonID  uniq   knows               uniq   100020  uniq   1           \n",
       " 100004  PersonID  uniq   knows               uniq   100021  uniq   1           \n",
       " 100002  PersonID  uniq   isFromCity          ambi   null    null   1           \n",
       " 100004  PersonID  uniq   isFromCity          ambi   null    null   1           \n",
       " 100006  PersonID  uniq   isFromCity          ambi   null    null   1           \n",
       "                                                                        \n",
       " 100006  PersonID  uniq   hasCitationScore    valu   null    null   1           \n",
       " 100002  PersonID  uniq   hasName             ambi   null    null   IN_TEMPLATE \n",
       " 100004  PersonID  uniq   hasName             ambi   null    null   IN_TEMPLATE \n",
       " 100006  PersonID  uniq   hasName             ambi   null    null   IN_TEMPLATE \n",
       " 100000  null      ambi   hasEntryCount       ambi   null    null   IN_TEMPLATE \n",
       ""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'{ofv_simple.uid_lu=}')\n",
    "ofv_simple.df_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2535 files...\n",
      "  50 | json 0.001s | ontology 0.018s ...\n",
      " 100 | json 0.001s | ontology 0.042s ...\n",
      "\n",
      "101 files processed\n",
      "json parse (per file):     0.001s | total: 0.068s\n",
      "ontology parse (per file): 0.043s | total: 4.309s\n"
     ]
    }
   ],
   "source": [
    "# Length\n",
    "#print(len(parse('$[*]').find(_json_)))\n",
    "\n",
    "# Examples\n",
    "#jsp_expr = parse('$[*].name')\n",
    "#jsp_expr = parse('$[*].cast.[*].name') # but note that it doesn't distinguish the movie id\n",
    "#jsp_expr = parse('$..director.name')\n",
    "#jsp_expr = parse('$..name')\n",
    "#jsp_expr = parse('$..genre[*]')\n",
    "#[match.value for match in jsp_expr.find(_json_)][:3]\n",
    "\n",
    "# IMDB 600K Transform Map\n",
    "# ... maybe add \"@@@\" for grouping the triples together ... and then \"^^^\" for sourcing?\n",
    "_xform_map_ = '''\n",
    "__id__              = '$[*]._id'              | MovieID      | uniq\n",
    "__director__        = '$[*].director.name_id' | DirectorID   | uniq\n",
    "__castmember__      = '$[*].cast.[*].name_id' | CastMemberID | uniq\n",
    "__id__              --- \"hasTitle\"       --- '$[*].name'          | xsd:string          ^^^ \"imdb_600k_international_movies\"\n",
    "__id__              --- \"yearReleased\"   --- '$[*].year'          | xsd:date     | yyyy ^^^ \"imdb_600k_international_movies\"\n",
    "__id__              --- \"runTime\"        --- '$[*].runtime'       | xsd:duration | dura ^^^ \"imdb_600k_international_movies\"\n",
    "__id__              --- \"hasGenre\"       --- '$[*].genre[*]'      | xsd.string   | cata ^^^ \"imdb_600k_international_movies\"\n",
    "__id__              --- \"ratingValue\"    --- '$[*].ratingValue'   | xsd:float    | valu ^^^ \"imdb_600k_international_movies\"\n",
    "__id__              --- \"summary\"        --- '$[*].summary_text'  | xsd:string   | cont ^^^ \"imdb_600k_international_movies\"\n",
    "__director__        --- \"directedMovie\"  --- __id__                                     ^^^ \"imdb_600k_international_movies\"\n",
    "__director__        --- \"hasName\"        --- '$[*].director.name' | xsd:string          ^^^ \"imdb_600k_international_movies\"\n",
    "__castmember__      --- \"castMemberOf\"   --- __id__                                     ^^^ \"imdb_600k_international_movies\"\n",
    "__castmember__      --- \"hasName\"        --- '$[*].cast.[*].name' | xsd:string          ^^^ \"imdb_600k_international_movies\"\n",
    "'''\n",
    "\n",
    "#\n",
    "# Updated Transform Map\n",
    "# ... the Person ID edge is now simplified to use the end jsonpath syntax\n",
    "# ... which isn't supported by the self modifying code example...\n",
    "#\n",
    "_xform_map_CONTAINS_REVERSE_ISSUE = '''\n",
    "__id__              = '$[*]._id'              | MovieID  | uniq\n",
    "__director__        = '$[*].director.name_id' | PersonID | uniq\n",
    "__castmember__      = '$[*].cast.[*].name_id' | PersonID | uniq\n",
    "__id__                       --- \"hasTitle\"       --- '$[*].name'          | xsd:string\n",
    "__id__                       --- \"yearReleased\"   --- '$[*].year'          | xsd:date\n",
    "__id__                       --- \"runTime\"        --- '$[*].runtime'       | xsd:duration\n",
    "__id__                       --- \"hasGenre\"       --- '$[*].genre[*]'      | xsd.string\n",
    "__id__                       --- \"ratingValue\"    --- '$[*].ratingValue'   | xsd:float\n",
    "__id__                       --- \"summary\"        --- '$[*].summary_text'  | xsd:string\n",
    "__director__                 --- \"directedMovie\"  --- __id__\n",
    "__castmember__               --- \"castMemberOf\"   --- __id__\n",
    "$..name_id | PersonID | uniq --- \"hasName\"        --- '$..name'            | xsd:string\n",
    "'''\n",
    "\n",
    "ofv = RTOntology(_xform_map_)\n",
    "_base_ = '../../../data/kaggle_imdb_600k/international-movies-json/'\n",
    "_files_ = os.listdir(_base_)\n",
    "print(f'{len(_files_)} files...')\n",
    "jsonparse_time_sum = ontology_time_sum = files_processed = 0\n",
    "for i in range(len(_files_)):\n",
    "    _file_ = _files_[i]\n",
    "    if (i > 0) and ((i % 50) == 0): print(f'{i:4} | json {jsonparse_time_sum/files_processed:0.3f}s | ontology {ontology_time_sum/files_processed:0.3f}s ...')\n",
    "    _txt_  = open(_base_ + _file_).read()\n",
    "    ts0 = time.time()\n",
    "    _json_ = json.loads(_txt_)\n",
    "    ts1 = time.time()        \n",
    "    ofv.parse(_json_)\n",
    "    ts2 = time.time()\n",
    "    jsonparse_time_sum += (ts1 - ts0)\n",
    "    ontology_time_sum  += (ts2 - ts1)\n",
    "    files_processed    += 1\n",
    "    if files_processed > 100: break\n",
    "\n",
    "print()\n",
    "print(f'{files_processed} files processed')\n",
    "print(f'json parse (per file):     {jsonparse_time_sum/files_processed:0.3f}s | total: {jsonparse_time_sum:0.3f}s')\n",
    "print(f'ontology parse (per file): {ontology_time_sum/files_processed:0.3f}s | total: {ontology_time_sum:0.3f}s')\n",
    "\n",
    "# just the first 10 files...\n",
    "# ... for all 10 template rows, it's 14.5s per file...  triples extracted is 36,547\n",
    "# ... cut down to 2 files... it's 4.8s per file after implementing the \"equal stars\" stub\n",
    "# ... ... however, to get the nulls to show up, you have to use find_or_create() from jsonpath-ng ...\n",
    "# ... ... and that creation sticks in an empty dictionary \"{}\" instead of a None...\n",
    "# ... ... with the self modifying code modification... now down to 0.015s per file\n",
    "# ... ... but this increases as more and more files are parsed... at 500 files, the average is as 0.24s / file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(ofv.df_triples)=361207\n",
      "len(ofv.uid_lu)=361841\n",
      "len(ofv.rev_uid_lu)=102207\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sbj</th><th>stype</th><th>sdisp</th><th>vrb</th><th>obj</th><th>otype</th><th>odisp</th><th>grp</th><th>gdisp</th><th>src</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>null</td><td>null</td><td>str</td></tr></thead><tbody><tr><td>214112</td><td>&quot;DirectorID&quot;</td><td>&quot;uniq&quot;</td><td>&quot;directedMovie&quot;</td><td>212205</td><td>&quot;MovieID&quot;</td><td>&quot;uniq&quot;</td><td>null</td><td>null</td><td>&quot;imdb_600k_inte</td></tr><tr><td>272357</td><td>&quot;MovieID&quot;</td><td>&quot;uniq&quot;</td><td>&quot;summary&quot;</td><td>273854</td><td>&quot;xsd:string&quot;</td><td>&quot;cont&quot;</td><td>null</td><td>null</td><td>&quot;imdb_600k_inte</td></tr><tr><td>114400</td><td>&quot;MovieID&quot;</td><td>&quot;uniq&quot;</td><td>&quot;yearReleased&quot;</td><td>114661</td><td>&quot;xsd:date&quot;</td><td>&quot;yyyy&quot;</td><td>null</td><td>null</td><td>&quot;imdb_600k_inte</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 10)\n",
       "\n",
       " sbj     stype       sdisp  vrb              odisp  grp   gdisp  src                     \n",
       " ---     ---         ---    ---               ---    ---   ---    ---                     \n",
       " i64     str         str    str               str    null  null   str                     \n",
       "\n",
       " 214112  DirectorID  uniq   directedMovie    uniq   null  null   imdb_600k_international \n",
       "                                                                  _movies                 \n",
       " 272357  MovieID     uniq   summary          cont   null  null   imdb_600k_international \n",
       "                                                                  _movies                 \n",
       " 114400  MovieID     uniq   yearReleased     yyyy   null  null   imdb_600k_international \n",
       "                                                                  _movies                 \n",
       ""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'{len(ofv.df_triples)=}')\n",
    "print(f'{len(ofv.uid_lu)=}')\n",
    "print(f'{len(ofv.rev_uid_lu)=}')\n",
    "ofv.df_triples.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_uid_ = 284569\n",
    "#print(f'{ofv.uid_lu[_uid_]=}')\n",
    "#_tuple_ =ofv.uid_lu[_uid_]\n",
    "#_key_   = str(_tuple_[0])+'|'+str(_tuple_[1])\n",
    "#print(f'{ofv.rev_uid_lu[_key_]=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mofv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimdb_600k_movies\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 309\u001b[0m, in \u001b[0;36mRTOntology.to_files\u001b[0;34m(self, _base_name_)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_files\u001b[39m(\u001b[38;5;28mself\u001b[39m, _base_name_):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_triples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_base_name_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.triples.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    310\u001b[0m     _lu_ \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muid\u001b[39m\u001b[38;5;124m'\u001b[39m:[], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt0\u001b[39m\u001b[38;5;124m'\u001b[39m:[], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt1\u001b[39m\u001b[38;5;124m'\u001b[39m:[], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt2\u001b[39m\u001b[38;5;124m'\u001b[39m:[]}\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _uid_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid_lu:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_parquet'"
     ]
    }
   ],
   "source": [
    "ofv.to_files('imdb_600k_movies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  $[*]._id                              :      300      300\n",
      "  $[*].genre[*]                         :      300      300\n"
     ]
    }
   ],
   "source": [
    "my_star_set = {'$[*]._id', '$[*].cast.[*].name'}\n",
    "my_star_set = ['$[*]._id', '$[*].name']\n",
    "my_star_set = ['$[*]._id', '$[*].genre[*]']\n",
    "\n",
    "golden = fillJSONPathElementsByJSONPath(my_star_set, _json_)\n",
    "filled = fillJSONPathElements          (my_star_set, _json_)\n",
    "for v in filled.keys(): print(f'  {v:38}: {len(filled[v]):8} {len(golden[v]):8}')\n",
    "for v in golden.keys():\n",
    "    for i in range(len(golden[v])):\n",
    "        if filled[v][i] != golden[v][i]: print(f'WRONG:  {v}: {filled[v][i]} != {golden[v][i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$[*]._id', '$[*].name']\n",
      "  $[*]._id                              :      250 | 0.00s \t\t      250 | 0.00s\n",
      "  $[*].name                             :      250 | 0.00s \t\t      250 | 0.00s\n",
      "['$[*]._id', '$[*].year']\n",
      "  $[*]._id                              :      250 | 0.00s \t\t      250 | 0.00s\n",
      "  $[*].year                             :      250 | 0.00s \t\t      250 | 0.00s\n",
      "['$[*]._id', '$[*].runtime']\n",
      "  $[*]._id                              :      250 | 0.00s \t\t      250 | 0.00s\n",
      "  $[*].runtime                          :      250 | 0.00s \t\t      250 | 0.00s\n",
      "['$[*]._id', '$[*].genre[*]']\n",
      "  $[*]._id                              :      300 | 0.00s \t\t      300 | 1.59s\n",
      "  $[*].genre[*]                         :      300 | 0.00s \t\t      300 | 1.59s\n",
      "['$[*]._id', '$[*].ratingValue']\n",
      "  $[*]._id                              :      250 | 0.00s \t\t      250 | 0.00s\n",
      "  $[*].ratingValue                      :      250 | 0.00s \t\t      250 | 0.00s\n",
      "['$[*]._id', '$[*].summary_text']\n",
      "  $[*]._id                              :      250 | 0.00s \t\t      250 | 0.00s\n",
      "  $[*].summary_text                     :      250 | 0.00s \t\t      250 | 0.00s\n",
      "['$[*].director.name_id', '$[*]._id']\n",
      "  $[*].director.name_id                 :      250 | 0.00s \t\t      250 | 0.00s\n",
      "  $[*]._id                              :      250 | 0.00s \t\t      250 | 0.00s\n",
      "  Incorrect: values differ @ 8 : {} != None\n",
      "['$[*].cast.[*].name_id', '$[*]._id']\n",
      "  $[*].cast.[*].name_id                 :      557 | 0.00s \t\t      557 | 2.48s\n",
      "  $[*]._id                              :      557 | 0.00s \t\t      557 | 2.48s\n",
      "['$[*].director.name_id', '$[*].director.name']\n",
      "  $[*].director.name_id                 :      250 | 0.00s \t\t      250 | 0.00s\n",
      "  $[*].director.name                    :      250 | 0.00s \t\t      250 | 0.00s\n",
      "  Incorrect: values differ @ 8 : {} != None\n",
      "  Incorrect: values differ @ 8 : {} != None\n",
      "['$[*].cast.[*].name_id', '$[*].cast.[*].name']\n",
      "  $[*].cast.[*].name_id                 :      557 | 0.00s \t\t      557 | 0.01s\n",
      "  $[*].cast.[*].name                    :      557 | 0.00s \t\t      557 | 0.01s\n"
     ]
    }
   ],
   "source": [
    "_examples_ = [['$[*]._id',                       '$[*].name'],\n",
    "              ['$[*]._id',                       '$[*].year'],\n",
    "              ['$[*]._id',                       '$[*].runtime'],\n",
    "              ['$[*]._id',                       '$[*].genre[*]'],\n",
    "              ['$[*]._id',                       '$[*].ratingValue'],\n",
    "              ['$[*]._id',                       '$[*].summary_text'],\n",
    "              ['$[*].director.name_id',          '$[*]._id'],\n",
    "              ['$[*].cast.[*].name_id',          '$[*]._id'],\n",
    "              ['$[*].director.name_id',          '$[*].director.name'],\n",
    "              ['$[*].cast.[*].name_id',          '$[*].cast.[*].name']]\n",
    "for _example_ in _examples_:\n",
    "    print(_example_)\n",
    "    ts0 = time.time()\n",
    "    golden = fillJSONPathElementsByJSONPath(_example_, _json_)\n",
    "    ts1 = time.time()\n",
    "    filled = fillJSONPathElements          (_example_, _json_)\n",
    "    ts2 = time.time()\n",
    "    for v in filled.keys(): print(f'  {v:38}: {len(filled[v]):8} | {(ts2-ts1):0.2f}s \\t\\t {len(golden[v]):8} | {(ts1-ts0):0.2f}s')\n",
    "    for v in golden.keys():\n",
    "        for i in range(len(golden[v])):\n",
    "            results_differ = False\n",
    "            if   i >= len(filled[v]):          results_differ, reason = True, 'lengths differ'\n",
    "            elif filled[v][i] != golden[v][i]: results_differ, reason = True, 'values differ @ '+str(i) + ' : ' + str(filled[v][i]) + ' != ' + str(golden[v][i])\n",
    "            if results_differ: break\n",
    "        if results_differ: print(f'  Incorrect: {reason}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$.people[*].id', '$.people[*].last', '$.people[*].first']\n",
      "  $.people[*].id                        :        3\n",
      "  $.people[*].last                      :        3\n",
      "  $.people[*].first                     :        3\n",
      "   $.people[*].id $.people[*].last $.people[*].first\n",
      "0              10            Smith              John\n",
      "1              20            Smith               Joe\n",
      "2              30            Jones              Mary\n",
      "['$.people[*].id', '$.people[*].age']\n",
      "  $.people[*].id                        :        3\n",
      "  $.people[*].age                       :        3\n",
      "   $.people[*].id  $.people[*].age\n",
      "0              10               30\n",
      "1              20               35\n",
      "2              30               32\n",
      "['$.people[*].id', '$.people[*].city', '$.people[*].state', '$.people[*].country']\n",
      "  $.people[*].id                        :        3\n",
      "  $.people[*].city                      :        3\n",
      "  $.people[*].state                     :        3\n",
      "  $.people[*].country                   :        3\n",
      "   $.people[*].id $.people[*].city $.people[*].state $.people[*].country\n",
      "0              10              nyc                ny                  us\n",
      "1              20             None                ny                  us\n",
      "2              30     philadelphia                pa                  us\n",
      "['$.people[*].id', '$.people[*].city']\n",
      "  $.people[*].id                        :        3\n",
      "  $.people[*].city                      :        3\n",
      "   $.people[*].id $.people[*].city\n",
      "0              10              nyc\n",
      "1              20             None\n",
      "2              30     philadelphia\n",
      "['$.knowsFrom[*][0]', '$.knowsFrom[*][1]', '$.knowsFrom[*][2]']\n",
      "  $.knowsFrom[*][0]                     :        2\n",
      "  $.knowsFrom[*][1]                     :        2\n",
      "  $.knowsFrom[*][2]                     :        2\n",
      "   $.knowsFrom[*][0]  $.knowsFrom[*][1] $.knowsFrom[*][2]\n",
      "0                 10                 20      Conference A\n",
      "1                 20                 30      Conference B\n"
     ]
    }
   ],
   "source": [
    "_json_txt_ = '''\n",
    "{\"id\":1,\n",
    " \"people\":[{\"first\":\"John\", \"last\":\"Smith\", \"id\":10, \"age\":30, \"city\":\"nyc\",          \"state\":\"ny\", \"country\":\"us\"},\n",
    "           {\"first\":\"Joe\",  \"last\":\"Smith\", \"id\":20, \"age\":35,                        \"state\":\"ny\", \"country\":\"us\"},\n",
    "           {\"first\":\"Mary\", \"last\":\"Jones\", \"id\":30, \"age\":32, \"city\":\"philadelphia\", \"state\":\"pa\", \"country\":\"us\"}],\n",
    " \"knowsFrom\":[[10, 20, \"Conference A\"], \n",
    "              [20, 30, \"Conference B\"]],\n",
    " \"education\":[{\"id\":10, \"degreeReceived\":\"Ph.D. in Computer Science\",   \"university\":\"Stanford University\"},\n",
    "              {\"id\":10, \"degreeReceived\":\"Masters in Computer Science\", \"university\":\"University of Pennsylvania\"}],\n",
    " \"total_people\":3\n",
    "}'''\n",
    "_json_simple_  = json.loads(_json_txt_)\n",
    "_examples_     = [\n",
    "['$.people[*].id',    '$.people[*].last', '$.people[*].first'],\n",
    "['$.people[*].id',    '$.people[*].age'],\n",
    "['$.people[*].id',    '$.people[*].city', '$.people[*].state', '$.people[*].country'],\n",
    "['$.people[*].id',    '$.people[*].city'],\n",
    "['$.knowsFrom[*][0]', '$.knowsFrom[*][1]', '$.knowsFrom[*][2]'],\n",
    "]\n",
    "for _example_ in _examples_:\n",
    "    print(_example_)\n",
    "    filled = fillJSONPathElements(_example_, _json_simple_)\n",
    "    for v in filled: print(f'  {v:38}: {len(filled[v]):8}')\n",
    "    print(pd.DataFrame(filled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
