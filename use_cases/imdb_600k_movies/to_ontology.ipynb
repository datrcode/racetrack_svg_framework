{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, '../../framework')\n",
    "from racetrack import *\n",
    "rt = RACETrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from jsonpath_ng import jsonpath, parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scanForward() - finds the next unescaped version of c in x starting at i\n",
    "def scanForward(x, i, c):\n",
    "    in_escape = False\n",
    "    while i < len(x):\n",
    "        if   x[i] == '\\\\' and in_escape == False: in_escape = True\n",
    "        else:\n",
    "            if x[i] == c and in_escape == False: return i\n",
    "            in_escape = False\n",
    "        i += 1\n",
    "    return None\n",
    "\n",
    "# literalize() - converts any single or double quoted strings into unique literal names\n",
    "def literalize(x):\n",
    "    l, lu = [], {}\n",
    "    i = 0\n",
    "    while i < len(x):\n",
    "        c = x[i]\n",
    "        if   c == \"'\":\n",
    "            j = scanForward(x, i+1, \"'\")\n",
    "            if j is None: raise Exception(f'OntologyForViz.literalize() - unterminated string literal \"{x}\"')\n",
    "            _literal_name_ = f'____lit{len(lu.keys())}____' # Surely, no one would ever use four underscores in a literal... and don't call me Surely\n",
    "            lu[_literal_name_] = x[i+1:j]\n",
    "            l.append(_literal_name_)\n",
    "            i = j + 1\n",
    "        elif c == '\"':\n",
    "            j = scanForward(x, i+1, '\"')\n",
    "            if j is None: raise Exception(f'OntologyForViz.literalize() - unterminated string literal \"{x}\"')\n",
    "            _literal_name_ = f'____lit{len(lu.keys())}____' # Surely, no one would ever use four underscores in a literal... and don't call me Surely\n",
    "            lu[_literal_name_] = x[i+1:j]\n",
    "            l.append(_literal_name_)\n",
    "            i = j + 1\n",
    "        else:\n",
    "            l.append(c)\n",
    "            i += 1\n",
    "    return ''.join(l), lu\n",
    "\n",
    "# fillLiteratals() - fill in the literal values (opposite of literalize but not guaranteed to keep spaces)\n",
    "def fillLiterals(x, lu):\n",
    "    for k, v in lu.items():\n",
    "        x = x.replace(k, v)\n",
    "    return x\n",
    "\n",
    "# findClosingParen() - find the next closing paren taking other open/closes into consideration\n",
    "# ... requires that literals were taken care of... [see literalize function]\n",
    "def findClosingParen(s, i):\n",
    "    stack = 0\n",
    "    while i < len(s):\n",
    "        if   s[i] == '(':               stack += 1\n",
    "        elif s[i] == ')' and stack > 0: stack -= 1\n",
    "        elif s[i] == ')':               return i\n",
    "        i += 1\n",
    "    raise Exception(f'OntologyForViz.findClosingParen() - no closing paren found for \"{s}\"')\n",
    "\n",
    "# tokenizeParameters() - create a token list of function parameters\n",
    "# ... requires that literals were taken care of... [see literalize function]\n",
    "def tokenizeParameters(x):\n",
    "    r = []\n",
    "    while ',' in x or '(' in x:\n",
    "        if   ',' in x and '(' in x: # both... process the one that occurs first\n",
    "            i = x.index(',')\n",
    "            j = x.index('(')\n",
    "            if i < j:\n",
    "                r.append(x[:i])\n",
    "                x = x[i+1:]\n",
    "            else:\n",
    "                k = findClosingParen(x,j+1)\n",
    "                r.append(x[:k+1].strip())\n",
    "                x = x[k+1:]\n",
    "                if ',' in x: x = x[x.index(',')+1:] # if there's another comma, consume it\n",
    "        elif ',' in x: # just literals from here on out...\n",
    "            r.append(x[:x.index(',')].strip())\n",
    "            x = x[x.index(',')+1:]\n",
    "        elif '(' in x: # just one function call from here on out...\n",
    "            i = x.index('(')\n",
    "            j = findClosingParen(x,i+1)\n",
    "            r.append(x[:j+1].strip())\n",
    "            x = x[j+1:]\n",
    "            if ',' in x: x = x[x.index(',')+1:] # if there's another comma, consume it\n",
    "    x = x.strip()\n",
    "    if len(x) > 0:\n",
    "        r.append(x)\n",
    "    return r\n",
    "\n",
    "# parseTree() - create a parse tree representation of a ontology node description\n",
    "def parseTree(x, node_value=None, node_children=None, node_name=None, lit_lu=None):\n",
    "    if node_value is None:\n",
    "        node_value = {}\n",
    "        node_children = {}\n",
    "        node_name = 'root'\n",
    "\n",
    "    if lit_lu is None:\n",
    "        x, lit_lu = literalize(x)\n",
    "    if '(' in x:\n",
    "        i          = x.index('(')\n",
    "        j          = findClosingParen(x, i+1)\n",
    "        fname      = x[0:i].strip()\n",
    "        parms      = tokenizeParameters(x[i+1:j])\n",
    "        node_value   [node_name] = lit_lu[fname] if fname in lit_lu else fname\n",
    "        node_children[node_name] = []    # functions have children... even if it's an empty list of children\n",
    "        for child_i in range(len(parms)):\n",
    "            child_name = f'{node_name}.{child_i}'\n",
    "            node_children[node_name].append(child_name)\n",
    "            parseTree(parms[child_i], node_value, node_children, child_name, lit_lu)\n",
    "    else:\n",
    "        x                        = x.strip()\n",
    "        node_value   [node_name] = lit_lu[x] if x in lit_lu else x\n",
    "        node_children[node_name] = None # literals have no children\n",
    "    return node_value, node_children\n",
    "\n",
    "# solveParseTree() - evaluate a parse tree\n",
    "def solveParseTree(values, children, filled, i, node=None):\n",
    "    if node is None: node = 'root'\n",
    "    if   children[node] is None and isJsonPath(values[node]):\n",
    "        return filled[values[node]][i]  # jsonpath filled in value from the json\n",
    "    elif children[node] is None:\n",
    "        return values[node]             # constant / literal\n",
    "    else:\n",
    "        parms = [solveParseTree(values, children, filled, i, x) for x in children[node]]\n",
    "        return eval(f'{values[node]}(*parms)')\n",
    "\n",
    "# upToStar() - upto the cth '[*]'\n",
    "def upToStar(x, c):\n",
    "    i = 0\n",
    "    while c > 0:\n",
    "        j = x.index('[*]', i)\n",
    "        i = j + 3\n",
    "        c -= 1\n",
    "    return x[:i]\n",
    "\n",
    "# fillStars() - fill the the stars in the specified order\n",
    "def fillStars(x, i, j=None, k=None):\n",
    "    if '[*]' not in x: return x # for example ... \"$.id\"\n",
    "    _index_ = x.index('[*]')\n",
    "    x = x[:_index_] + f'[{i}]' + x[_index_+3:]\n",
    "    if j is not None and '[*]' in x:\n",
    "        _index_ = x.index('[*]')\n",
    "        x = x[:_index_] + f'[{j}]' + x[_index_+3:]\n",
    "    if k is not None and '[*]' in x:\n",
    "        _index_ = x.index('[*]')\n",
    "        x = x[:_index_] + f'[{k}]' + x[_index_+3:]\n",
    "    return x\n",
    "\n",
    "# isJsonPath() - check if the string is a jsonpath\n",
    "def isJsonPath(_str_): \n",
    "    return _str_.startswith('$.') or _str_.startswith('$[')\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "class RTOntology(object):\n",
    "    # __init__() - prepare transform spec for use and initial instance variables\n",
    "    def __init__(self, xform_spec):\n",
    "        self.xform_spec_lines = self.__substituteDefines__(xform_spec)\n",
    "        self.df_triples = None\n",
    "        self.uid_lu     = {}\n",
    "        self.rev_uid_lu = {}\n",
    "\n",
    "    # __substituteDefines__() - subsitute defines\n",
    "    def __substituteDefines__(self, _txt_):\n",
    "        lines     = _txt_.split('\\n')\n",
    "        subs      = {}\n",
    "        completes = []\n",
    "        for _line_ in lines:\n",
    "            tokens = _line_.split()\n",
    "            if len(tokens) >= 3 and tokens[1] == '=':\n",
    "                subs[tokens[0]] = ' '.join(tokens[2:])\n",
    "            else:\n",
    "                for r in subs:\n",
    "                    if r in _line_:\n",
    "                        _line_ = _line_.replace(r, subs[r])\n",
    "                if len(_line_) > 0:\n",
    "                    completes.append(_line_)\n",
    "        return completes\n",
    "\n",
    "    # __applyTemplate__() - apply templated line in the transform to the json representation\n",
    "    def __applyTemplate__(self, \n",
    "                          myjson,        # json representation\n",
    "                          s_values,      s_children,    s_type,     s_uniq, # subject params\n",
    "                          v_values,      v_children,                        # verb params   (it's only a string, no typing, unique to the schema)\n",
    "                          o_values,      o_children,    o_type,     o_uniq, # object params\n",
    "                          g_values,      g_children,    g_type,     g_uniq, # group params\n",
    "                          src_values,    src_children,                      # source params (it's only a string, no typing, unique to this ontological instance)\n",
    "                          ):\n",
    "        # resolve the jsonpath values        \n",
    "        all_values  = set(s_values.values()) | set(v_values.values()) | set(o_values.values())\n",
    "        if g_values   is not None: all_values |= set(g_values.values())\n",
    "        if src_values is not None: all_values |= set(src_values.values())\n",
    "\n",
    "        path_values, longest_by_star_path, filled = [], None, {}\n",
    "        for x in all_values:\n",
    "            filled[x] = []\n",
    "            if isJsonPath(x):\n",
    "                path_values.append(x)\n",
    "                if '*' in x:\n",
    "                    if   longest_by_star_path is None:\n",
    "                        longest_by_star_path = x\n",
    "                    elif longest_by_star_path.rindex('*') < x.rindex('*'):\n",
    "                        longest_by_star_path = x\n",
    "\n",
    "        # ensure that all jsonpath values are substrings of the longest star path\n",
    "        for x in path_values:\n",
    "            if '*' in x:\n",
    "                x_until_last_star = x[:x.rindex('*')+2] # get the close bracket too\n",
    "                if longest_by_star_path[:len(x_until_last_star)] != x_until_last_star:\n",
    "                    raise Exception(f'OntologyForViz.__applyTemplate__() - jsonpath are not subsets \"{x}\" vs \"{longest_by_star_path}\"')\n",
    "                \n",
    "        # fill in the json values into the filled dict\n",
    "        if    longest_by_star_path is None:\n",
    "            raise('OntologyForViz.__applyTemplate__() - no meaningful jsonpath(s) found')\n",
    "        else:\n",
    "            star_count = longest_by_star_path.count('[*]')\n",
    "            if star_count   == 1:\n",
    "                for i in range(len(parse(upToStar(longest_by_star_path, 1)).find(myjson))):\n",
    "                    for v in filled.keys():\n",
    "                        if isJsonPath(v):\n",
    "                            _matches_ = parse(fillStars(v, i)).find(myjson)\n",
    "                            if len(_matches_) == 1: filled[v].append(_matches_[0].value)\n",
    "                            else:                   filled[v].append(None)\n",
    "                        else:\n",
    "                            filled[v].append(v)\n",
    "            elif star_count == 2:\n",
    "                for i in range(len(parse(upToStar(longest_by_star_path, 1)).find(myjson))):\n",
    "                    for j in range(len(parse(upToStar(fillStars(longest_by_star_path,i), 1)).find(myjson))):\n",
    "                        for v in filled.keys():\n",
    "                            if isJsonPath(v):\n",
    "                                _matches_ = parse(fillStars(v, i, j)).find(myjson)\n",
    "                                if len(_matches_) == 1: filled[v].append(_matches_[0].value)\n",
    "                                else:                   filled[v].append(None)\n",
    "                            else:\n",
    "                                filled[v].append(v)\n",
    "            elif star_count == 3:\n",
    "                for i in range(len(parse(upToStar(longest_by_star_path, 1)).find(myjson))):\n",
    "                    for j in range(len(parse(upToStar(fillStars(longest_by_star_path,i), 1)).find(myjson))):\n",
    "                        for k in range(len(parse(upToStar(fillStars(longest_by_star_path,i,j), 1)).find(myjson))):\n",
    "                            for v in filled.keys():\n",
    "                                if isJsonPath(v):\n",
    "                                    _matches_ = parse(fillStars(v, i, j, k)).find(myjson)\n",
    "                                    if len(_matches_) == 1: filled[v].append(_matches_[0].value)\n",
    "                                    else:                   filled[v].append(None)\n",
    "                                else:\n",
    "                                    filled[v].append(v)\n",
    "            else:\n",
    "                raise Exception(f'OntologyForViz.__applyTemplate__() - max of three stars supported -- {star_count} found')\n",
    "\n",
    "        # collapse the parse trees based on the filled values\n",
    "        # ... double check that they are the same length\n",
    "        l = None\n",
    "        for v in filled.keys():\n",
    "            if l is None: l = len(filled[v])\n",
    "            if len(filled[v]) != l: raise Exception(f'OntologyForViz.__applyTemplate__() - unequal number of values for {v}')\n",
    "        pre_df = {}\n",
    "        pre_df['sbj']    = [solveParseTree(s_values,   s_children,   filled, i) for i in range(l)]\n",
    "        pre_df['vrb']    = [solveParseTree(v_values,   v_children,   filled, i) for i in range(l)]\n",
    "        pre_df['obj']    = [solveParseTree(o_values,   o_children,   filled, i) for i in range(l)]\n",
    "        if g_values   is not None: pre_df['grp'] = [solveParseTree(g_values,   g_children,   filled, i) for i in range(l)]\n",
    "        if src_values is not None: pre_df['src'] = [solveParseTree(src_values, src_children, filled, i) for i in range(l)]\n",
    "\n",
    "        for_df = {'sbj': [], 'vrb': [], 'obj': [], 'grp':[], 'src':[]}\n",
    "        for i in range(l):\n",
    "            #\n",
    "            # Subject (Required)\n",
    "            #\n",
    "            _sbj_, _sbj_type_, _sbj_uniq_ = pre_df['sbj'][i], s_type, s_uniq\n",
    "            if type(_sbj_) == tuple:\n",
    "                _sbj_type_ = _sbj_[1] if len(_sbj_) > 1 else s_type\n",
    "                _sbj_uniq_ = _sbj_[2] if len(_sbj_) > 2 else s_uniq\n",
    "                _sbj_      = _sbj_[0]\n",
    "            _sbj_uid_ = self.resolveUniqIdAndUpdateLookups(_sbj_, _sbj_type_, _sbj_uniq_, 'sbj')\n",
    "            for_df['sbj'].append(_sbj_uid_)\n",
    "\n",
    "            #\n",
    "            # Verb (Required)\n",
    "            #\n",
    "            _vrb_ = pre_df['vrb'][i]\n",
    "            for_df['vrb'].append(_vrb_)\n",
    "\n",
    "            #\n",
    "            # Object (Required)\n",
    "            #\n",
    "            _obj_, _obj_type_, _obj_uniq_ = pre_df['obj'][i], o_type, o_uniq\n",
    "            if type(_obj_) == tuple:\n",
    "                _obj_type_ = _obj_[1] if len(_obj_) > 1 else o_type\n",
    "                _obj_uniq_ = _obj_[2] if len(_obj_) > 2 else o_uniq\n",
    "                _obj_      = _obj_[0]\n",
    "            _obj_uid_ = self.resolveUniqIdAndUpdateLookups(_obj_, _obj_type_, _obj_uniq_, 'obj')            \n",
    "            for_df['obj'].append(_obj_uid_)\n",
    "\n",
    "            #\n",
    "            # Grouping (Optional)\n",
    "            #\n",
    "            if g_values is not None:\n",
    "                _grp_, _grp_type_, _grp_uniq_ = pre_df['grp'][i], g_type, g_uniq\n",
    "                if type(_grp_) == tuple:\n",
    "                    _grp_type_ = _grp_[1] if len(_grp_) > 1 else g_type\n",
    "                    _grp_uniq_ = _grp_[2] if len(_grp_) > 2 else g_uniq\n",
    "                    _grp_      = _grp_[0]\n",
    "                _grp_uid_ = self.resolveUniqIdAndUpdateLookups(_grp_, _grp_type_, _grp_uniq_, 'grp')\n",
    "                for_df['grp'].append(_grp_uid_)\n",
    "            else:\n",
    "                for_df['grp'].append(None)\n",
    "\n",
    "            #\n",
    "            # Sourcing (Optional)\n",
    "            #\n",
    "            if src_values is not None:\n",
    "                _src_ = pre_df['src'][i]\n",
    "                for_df['src'].append(str(_src_))\n",
    "            else:\n",
    "                for_df['src'].append(None)\n",
    "\n",
    "        _df_            = pl.DataFrame(for_df)\n",
    "        self.df_triples = _df_ if self.df_triples is None else pl.concat([_df_, self.df_triples])\n",
    "\n",
    "    # resolveIdAndUpdateLookups() - resolve id and update lookups\n",
    "    # self.uid_lu[<interger>] = (id-from-input, type-from-input, uniq-from-input)\n",
    "    #\n",
    "    def resolveUniqIdAndUpdateLookups(self, _id_, _type_, _uniq_, _occurs_in_):\n",
    "        _uniq_key_ = str(_id_)+'|'+str(_type_)\n",
    "        if _uniq_ and _uniq_key_ in self.rev_uid_lu:\n",
    "            return self.rev_uid_lu[_uniq_key_]\n",
    "        my_uid = 100_000 + len(self.uid_lu.keys())\n",
    "        self.uid_lu[my_uid] = (_id_, _type_, _uniq_)\n",
    "        if _uniq_:  self.rev_uid_lu[_uniq_key_] = my_uid\n",
    "        return my_uid\n",
    "\n",
    "    # parse() - parse json into ontology via specification\n",
    "    def parse(self, j):\n",
    "        for l in self.xform_spec_lines:\n",
    "            l, lu = literalize(l) # get rid of any literal values so it doesn't mess up the delimiters\n",
    "            if '#' in l: l = l[:l.index('#')].strip() # comments... hope the hash doesn't occur anywhere in the template that isn't a comment\n",
    "            if len(l) == 0: continue\n",
    "\n",
    "            # Sourcing Information\n",
    "            src_values = src_children = None\n",
    "            if '^^^' in l:\n",
    "                src = l[l.index('^^^')+3:]\n",
    "                l   = l[:l.index('^^^')].strip()\n",
    "                src_values, src_children = parseTree(fillLiterals(src, lu))\n",
    "\n",
    "            # Grouping Information\n",
    "            g_values = g_children = g_type = g_uniq = None\n",
    "            if '@@@' in l:\n",
    "                grp = l[l.index('@@@')+3:]\n",
    "                l   = l[:l.index('@@@')].strip()\n",
    "                g_uniq = None\n",
    "                if grp.endswith('uniq') and '|' in grp:\n",
    "                    grp = grp[:grp.rindex('|')]\n",
    "                    g_uniq = True\n",
    "                g_type = None\n",
    "                if '|' in grp:\n",
    "                    g_type = grp[grp.rindex('|')+1:].strip()\n",
    "                    grp   = grp[:grp.rindex('|')]\n",
    "                g_node = grp\n",
    "                g_values, g_children = parseTree(fillLiterals(g_node, lu))\n",
    "                \n",
    "            svo = [x.strip() for x in l.split('---')]\n",
    "            if len(svo) == 3:\n",
    "                s, v, o = svo[0], svo[1], svo[2]\n",
    "\n",
    "                # Subject\n",
    "                s_uniq = None\n",
    "                if s.endswith('uniq') and '|' in s:\n",
    "                    s = s[:s.rindex('|')]\n",
    "                    s_uniq = True\n",
    "                s_type = None\n",
    "                if '|' in s:\n",
    "                    s_type = s[s.rindex('|')+1:].strip()\n",
    "                    s      = s[:s.rindex('|')]\n",
    "                s_node = s\n",
    "                s_values, s_children = parseTree(fillLiterals(s_node, lu))\n",
    "\n",
    "                # Verb\n",
    "                v_values, v_children = parseTree(fillLiterals(v, lu))\n",
    "\n",
    "                # Object\n",
    "                o_uniq = None\n",
    "                if o.endswith('uniq') and '|' in o:\n",
    "                    o = o[:o.rindex('|')]\n",
    "                    o_uniq = True\n",
    "                if '|' in o:\n",
    "                    o_type = o[o.rindex('|')+1:].strip()\n",
    "                    o      = o[:o.rindex('|')]\n",
    "                o_node = o\n",
    "                o_values, o_children = parseTree(fillLiterals(o_node, lu))\n",
    "\n",
    "                self.__applyTemplate__(j, s_values, s_children, s_type, s_uniq, \n",
    "                                          v_values, v_children, \n",
    "                                          o_values, o_children, o_type, o_uniq,\n",
    "                                          g_values, g_children, g_type, g_uniq,\n",
    "                                          src_values, src_children)\n",
    "            else:\n",
    "                raise Exception(f'OntologyForViz.parse() - line \"{l}\" does not have three parts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_json_txt_ = '''\n",
    "{\"id\":1,\n",
    " \"people\":[{\"first\":\"John\", \"last\":\"Smith\", \"id\":10, \"age\":30, \"city\":\"nyc\",          \"state\":\"ny\", \"country\":\"us\"},\n",
    "           {\"first\":\"Joe\",  \"last\":\"Smith\", \"id\":20, \"age\":35,                        \"state\":\"ny\", \"country\":\"us\"},\n",
    "           {\"first\":\"Mary\", \"last\":\"Jones\", \"id\":30, \"age\":32, \"city\":\"philadelphia\", \"state\":\"pa\", \"country\":\"us\"}],\n",
    " \"knowsFrom\":[[10, 20, \"Conference A\"], \n",
    "              [20, 30, \"Conference B\"]],\n",
    " \"education\":[{\"id\":10, \"degreeReceived\":\"Ph.D. in Computer Science\",   \"university\":\"Stanford University\"},\n",
    "              {\"id\":10, \"degreeReceived\":\"Masters in Computer Science\", \"university\":\"University of Pennsylvania\"}],\n",
    " \"total_people\":3\n",
    "}'''\n",
    "_json_simple_  = json.loads(_json_txt_)\n",
    "def concatNames(_last_,_first_):\n",
    "    return _last_ + ' ' + _first_\n",
    "def combineAddress(_city_,_state_,_country_):\n",
    "    s = ''\n",
    "    if _city_    is not None: s += _city_\n",
    "    if _state_   is not None: s += ', ' + _state_    if (len(s) > 0) else _state_\n",
    "    if _country_ is not None: s += ', ' + _country_  if (len(s) > 0) else _country_\n",
    "    return s if (len(s) > 0) else 'Not Supplied'\n",
    "_xform_simple_ = '''\n",
    "'$.people[*].id'    | PersonID | uniq --- \"hasName\" --- concatNames('$.people[*].last', '$.people[*].first') | xsd:string                                                                             ^^^ \"IN_TEMPLATE\"\n",
    "'$.people[*].id'    | PersonID | uniq --- \"hasAge\"  --- '$.people[*].age' | xsd:integer                                                                                                               ^^^ '$.id'\n",
    "'$.people[*].id'    | PersonID | uniq --- \"isFrom\"  --- combineAddress('$.people[*].city', '$.people[*].state', '$.people[*].country') | CityStateCountry                                             ^^^ '$.id'\n",
    "'$.knowsFrom[*][0]' | PersonID | uniq --- \"knows\"   --- '$.knowsFrom[*][1]' | PersonID | uniq                                                             @@@ '$.knowsFrom[*][2]' | xsd:string | uniq ^^^ '$.id'\n",
    "'''\n",
    "ofv_simple = RTOntology(_xform_simple_)\n",
    "ofv_simple.parse(_json_simple_)\n",
    "print(ofv_simple.uid_lu)\n",
    "print(ofv_simple.df_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Length\n",
    "#\n",
    "#print(len(parse('$[*]').find(_json_)))\n",
    "#\n",
    "# Examples\n",
    "#\n",
    "#jsp_expr = parse('$[*].name')\n",
    "#jsp_expr = parse('$[*].cast.[*].name') # but note that it doesn't distinguish the movie id\n",
    "#jsp_expr = parse('$..director.name')\n",
    "#jsp_expr = parse('$..name')\n",
    "#jsp_expr = parse('$..genre[*]')\n",
    "#[match.value for match in jsp_expr.find(_json_)][:3]\n",
    "\n",
    "# IMDB 600K Transform Map\n",
    "# ... maybe add \"@@@\" for grouping the triples together ... and then \"^^^\" for sourcing?\n",
    "_xform_map_ = '''\n",
    "__id__              = '$[*]._id'              | MovieID      | uniq\n",
    "__director__        = '$[*].director.name_id' | DirectorID   | uniq\n",
    "__castmember__      = '$[*].cast.[*].name_id' | CastMemberID | uniq\n",
    "__id__              --- \"hasTitle\"       --- '$[*].name'          | xsd:string\n",
    "__id__              --- \"yearReleased\"   --- '$[*].year'          | xsd:date\n",
    "__id__              --- \"runTime\"        --- '$[*].runtime'       | xsd:duration\n",
    "__id__              --- \"hasGenre\"       --- '$[*].genre[*]'      | xsd.string\n",
    "__id__              --- \"ratingValue\"    --- '$[*].ratingValue'   | xsd:float\n",
    "__id__              --- \"summary\"        --- '$[*].summary_text'  | xsd:string\n",
    "__director__        --- \"directedMovie\"  --- __id__\n",
    "__director__        --- \"hasName\"        --- '$[*].director.name' | xsd:string\n",
    "__castmember__      --- \"castMemberOf\"   --- __id__\n",
    "__castmember__      --- \"hasName\"        --- '$[*].cast.[*].name' | xsd:string\n",
    "'''\n",
    "\n",
    "ofv = RTOntology(_xform_map_)\n",
    "_base_ = '../../../data/kaggle_imdb_600k/international-movies-json/'\n",
    "_files_ = os.listdir(_base_)\n",
    "print(f'{len(_files_)} files...')\n",
    "jsonparse_time_sum = ontology_time_sum = files_processed = 0\n",
    "for i in range(len(_files_)):\n",
    "    _file_ = _files_[i]\n",
    "    if (i > 0) and ((i % 500) == 0): print(f'{i} / json {jsonparse_time_sum/files_processed:0.1f}s / ontology {ontology_time_sum/files_processed:0.1f}s ...')\n",
    "    _txt_  = open(_base_ + _file_).read()\n",
    "    ts0 = time.time()\n",
    "    _json_ = json.loads(_txt_)\n",
    "    ts1 = time.time()        \n",
    "    ofv.parse(_json_)\n",
    "    ts2 = time.time()\n",
    "    jsonparse_time_sum += (ts1 - ts0)\n",
    "    ontology_time_sum  += (ts2 - ts1)\n",
    "    files_processed    += 1\n",
    "    if files_processed >= 10: break\n",
    "\n",
    "print()\n",
    "print(f'{files_processed} files processed')\n",
    "print(f'json parse (per file):     {jsonparse_time_sum/files_processed:0.1f}s')\n",
    "print(f'ontology parse (per file): {ontology_time_sum/files_processed:0.1f}s')\n",
    "\n",
    "# just the first 10 files...\n",
    "# ... for all 10 template rows, it's 14.5s per file...  triples extracted is 36,547\n",
    "\n",
    "print(len(ofv.df_triples))\n",
    "ofv.df_triples.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
