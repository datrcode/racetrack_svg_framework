{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "def promptModel(_user_, _system_='You are a helpful digital assistant.', max_tokens=256):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": _system_},\n",
    "        {\"role\": \"user\",   \"content\": _user_},\n",
    "    ]\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=max_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "_base_dir_ = '../../../data/2014_vast/MC1/News Articles'\n",
    "\n",
    "files_processed = 0\n",
    "_lu_       = {'file':[], 'article':[], 'llama3_8b_response':[], 'llama3_8b_time':[]}\n",
    "_prompt_   = '''Translate the following user text to an RDF graph using both the FOAF, and Schema1 ontologies.\n",
    "Use the prefix ex: with IRI <http://example.com/> for any created entities.\n",
    "'''\n",
    "\n",
    "for _dir_ in os.listdir(_base_dir_):\n",
    "    for _file_ in os.listdir(os.path.join(_base_dir_, _dir_)):\n",
    "        _article_raw_ = open(os.path.join(_base_dir_, _dir_, _file_), 'rb').read()\n",
    "        _article_     = str(_article_raw_) #.replace('\\\\r', '').split('\\\\n')\n",
    "        ts0_model = time.time()\n",
    "        _response_    = promptModel(_article_, _prompt_, max_tokens=4096)\n",
    "        ts1_model = time.time()\n",
    "\n",
    "        _lu_['file'].append(_file_)\n",
    "        _lu_['article'].append(_article_)\n",
    "        _lu_['llama3_8b_response'].append(_response_)\n",
    "        _lu_['llama3_8b_time'].append(ts1_model-ts0_model)\n",
    "\n",
    "        pd.DataFrame(_lu_).to_csv('llama3_8b_2014_vast_sbs.csv', index=False)\n",
    "        files_processed += 1\n",
    "        if files_processed > 1: break\n",
    "    if files_processed > 1: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(_lu_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_lu_['llama3_8b_response'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakIntoSentences(_str_):\n",
    "    _sentences_ = []\n",
    "    for _ in _str_.split('.'):\n",
    "        if len(_) > 0:\n",
    "            _sentences_.append(_.strip() + '.')\n",
    "    return _sentences_\n",
    "\n",
    "def separateArticle(_str_):\n",
    "    _source_, _title_, _published_, _sentences_ = '', '', '', []\n",
    "    for _ in _str_.split('\\n'):\n",
    "        if len(_) > 0 and _.startswith('<<') == False:\n",
    "            if   _.startswith(\"b'SOURCE:\"):     _source_  = _.replace(\"b'SOURCE: \", '')  .strip()\n",
    "            elif _.startswith(\"TITLE:\"):      _title_     = _.replace('TITLE: ', '')     .strip()\n",
    "            elif _.startswith(\"PUBLISHED:\"):  _published_ = _.replace('PUBLISHED: ', '') .strip()\n",
    "            else:                             _sentences_.extend(breakIntoSentences(_))\n",
    "    return _source_, _title_, _published_, _sentences_\n",
    "\n",
    "_src_, _title_, _published_, _sentences_ = separateArticle(_lu_['article'][1].replace('\\\\r', '').replace('\\\\n', '\\n'))\n",
    "_sentences_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prompt_   = '''Translate the following user text to an RDF graph using both the FOAF, and Schema1 ontologies.\n",
    "Use the prefix ex: with IRI <http://example.com/> for any created entities.\n",
    "'''\n",
    "_prompt_   = 'Translate the following text into an CCO ontology represented as JSON.  Only include the JSON structure.'\n",
    "_sentence_ = 'The army of people Asterian (APA) is paramilitary organization which has been busy with terrorist activities ' + \\\n",
    "             'which are financed by its criminal ventures, which include drug trafficking.'\n",
    "_response_ = promptModel(_sentence_, _prompt_, max_tokens=4096)\n",
    "print(_response_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
