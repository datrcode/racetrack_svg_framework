{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import rtsvg\n",
    "rt = rtsvg.RACETrack()\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "def promptModel(prompt, model):\n",
    "    response: ChatResponse = chat(model=model, messages=[{ 'role': 'user', 'content': prompt,},])\n",
    "    return response['message']['content']\n",
    "\n",
    "_dir_ = '../../../data/2014_vast/MC1/'\n",
    "df = pd.read_parquet(_dir_ + 'articles.parquet')\n",
    "def extractPythonBlock(s):\n",
    "    _start_, _end_ = '```python', '```'\n",
    "    if _start_ in s and _end_ in s[s.index(_start_)+len(_start_):]:\n",
    "        i0 = s.index(_start_)+len(_start_)\n",
    "        i1 = s.index(_end_, i0)\n",
    "        return s[i0:i1]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_file_deepseek_responses_ = _dir_ + '20250201_deep_seek_responses.parquet'\n",
    "if os.path.exists(_file_deepseek_responses_) == False:\n",
    "    _response_lu_, _timing_lu_ = {}, {}\n",
    "    _model_ = 'deepseek-r1:14b'\n",
    "    total_files     = df['file'].nunique()\n",
    "    files_processed = 0\n",
    "    for k, k_df in df.groupby('file'):\n",
    "        if k in _response_lu_:\n",
    "            files_processed += 1 \n",
    "            continue\n",
    "        k_df             = k_df.sort_values('sentence_no').reset_index()\n",
    "        _article_        = ''.join(k_df['sentence'])\n",
    "        t0               = time.time()\n",
    "        _response_       = promptModel(f'Extract all of the entities into a Python dictionary from the following article.  The dictionary pairing should indicate the entities type:\\n\\n{_article_}', _model_)\n",
    "        _timing_lu_[k]   = time.time() - t0\n",
    "        _response_lu_[k] = _response_\n",
    "        print('.', end='')\n",
    "        if files_processed % 10 == 0:\n",
    "            print(f'{files_processed}/{total_files}', end='')\n",
    "            _file_save_info_ = {'file':[], 'deep_seek_response':[], 'time_taken':[]}\n",
    "            for _keyvalue_ in _response_lu_:\n",
    "                _file_save_info_['file'].append(_keyvalue_)\n",
    "                _file_save_info_['deep_seek_response'].append(_response_lu_[_keyvalue_])\n",
    "                _file_save_info_['time_taken'].append(_timing_lu_[_keyvalue_])\n",
    "            df_intermediate = pd.DataFrame(_file_save_info_)\n",
    "            df_intermediate.to_parquet('INTERMEDIA_deep_seek_responses.parquet')\n",
    "        files_processed += 1\n",
    "    _file_save_info_ = {'file':[], 'deep_seek_response':[], 'time_taken':[]}\n",
    "    for _keyvalue_ in _response_lu_:\n",
    "        _file_save_info_['file'].append(_keyvalue_)\n",
    "        _file_save_info_['deep_seek_response'].append(_response_lu_[_keyvalue_])\n",
    "        _file_save_info_['time_taken'].append(_timing_lu_[_keyvalue_])\n",
    "    df_final = pd.DataFrame(_file_save_info_)\n",
    "    df_final.to_parquet(_file_deepseek_responses_)\n",
    "df_deepseek          = pd.read_parquet(_file_deepseek_responses_)\n",
    "df_deepseek          = df_deepseek.rename({'deep_seek_response':'model_response'}, axis=1)\n",
    "df_deepseek['model'] = 'deepseek-r1:14b'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_response_lu_, _timing_lu_ = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_file_llama32_responses_ = _dir_ + '20250201_llama32_responses.parquet'\n",
    "if os.path.exists(_file_llama32_responses_) == False:\n",
    "    _model_ = 'llama3.2'\n",
    "    total_files     = df['file'].nunique()\n",
    "    files_processed = 0\n",
    "    for k, k_df in df.groupby('file'):\n",
    "        if k in _response_lu_:\n",
    "            files_processed += 1 \n",
    "            continue\n",
    "        k_df             = k_df.sort_values('sentence_no').reset_index()\n",
    "        _article_        = ''.join(k_df['sentence'])\n",
    "        t0               = time.time()\n",
    "        _response_       = promptModel(f'Extract all of the entities into a Python dictionary from the following article.  The dictionary pairing should indicate the entities type:\\n\\n{_article_}', _model_)\n",
    "        _timing_lu_[k]   = time.time() - t0\n",
    "        _response_lu_[k] = _response_\n",
    "        print('.', end='')\n",
    "        if files_processed % 10 == 0:\n",
    "            print(f'{files_processed}/{total_files}', end='')\n",
    "            _file_save_info_ = {'file':[], 'llama32_response':[], 'time_taken':[]}\n",
    "            for _keyvalue_ in _response_lu_:\n",
    "                _file_save_info_['file'].append(_keyvalue_)\n",
    "                _file_save_info_['llama32_response'].append(_response_lu_[_keyvalue_])\n",
    "                _file_save_info_['time_taken'].append(_timing_lu_[_keyvalue_])\n",
    "            df_intermediate = pd.DataFrame(_file_save_info_)\n",
    "            df_intermediate.to_parquet('INTERMEDIA_llama32_responses.parquet')\n",
    "        files_processed += 1\n",
    "    _file_save_info_ = {'file':[], 'llama32_response':[], 'time_taken':[]}\n",
    "    for _keyvalue_ in _response_lu_:\n",
    "        _file_save_info_['file'].append(_keyvalue_)\n",
    "        _file_save_info_['llama32_response'].append(_response_lu_[_keyvalue_])\n",
    "        _file_save_info_['time_taken'].append(_timing_lu_[_keyvalue_])\n",
    "    df_final = pd.DataFrame(_file_save_info_)\n",
    "    df_final.to_parquet(_file_llama32_responses_)\n",
    "df_llama32          = pd.read_parquet(_file_llama32_responses_)\n",
    "df_llama32          = df_llama32.rename({'llama32_response':'model_response'}, axis=1)\n",
    "df_llama32['model'] = 'llama32'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_response_lu_, _timing_lu_ = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_file_phi4_responses_ = _dir_ + '20250201_phi4_responses.parquet'\n",
    "if os.path.exists(_file_phi4_responses_) == False:\n",
    "    _model_ = 'phi4'\n",
    "    total_files     = df['file'].nunique()\n",
    "    files_processed = 0\n",
    "    for k, k_df in df.groupby('file'):\n",
    "        if k in _response_lu_:\n",
    "            files_processed += 1 \n",
    "            continue\n",
    "        k_df             = k_df.sort_values('sentence_no').reset_index()\n",
    "        _article_        = ''.join(k_df['sentence'])\n",
    "        t0               = time.time()\n",
    "        _response_       = promptModel(f'Extract all of the entities into a Python dictionary from the following article.  The dictionary pairing should indicate the entities type:\\n\\n{_article_}', _model_)\n",
    "        _timing_lu_[k]   = time.time() - t0\n",
    "        _response_lu_[k] = _response_\n",
    "        print('.', end='')\n",
    "        if files_processed % 10 == 0:\n",
    "            print(f'{files_processed}/{total_files}', end='')\n",
    "            _file_save_info_ = {'file':[], 'phi4_response':[], 'time_taken':[]}\n",
    "            for _keyvalue_ in _response_lu_:\n",
    "                _file_save_info_['file'].append(_keyvalue_)\n",
    "                _file_save_info_['phi4_response'].append(_response_lu_[_keyvalue_])\n",
    "                _file_save_info_['time_taken'].append(_timing_lu_[_keyvalue_])\n",
    "            df_intermediate = pd.DataFrame(_file_save_info_)\n",
    "            df_intermediate.to_parquet('INTERMEDIA_phi4_responses.parquet')\n",
    "        files_processed += 1\n",
    "    _file_save_info_ = {'file':[], 'phi4_response':[], 'time_taken':[]}\n",
    "    for _keyvalue_ in _response_lu_:\n",
    "        _file_save_info_['file'].append(_keyvalue_)\n",
    "        _file_save_info_['phi4_response'].append(_response_lu_[_keyvalue_])\n",
    "        _file_save_info_['time_taken'].append(_timing_lu_[_keyvalue_])\n",
    "    df_final = pd.DataFrame(_file_save_info_)\n",
    "    df_final.to_parquet(_file_phi4_responses_)\n",
    "df_phi4          = pd.read_parquet(_file_phi4_responses_)\n",
    "df_phi4          = df_phi4.rename({'phi4_response':'model_response'}, axis=1)\n",
    "df_phi4['model'] = 'phi4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses                        = pd.concat([df_deepseek, df_llama32, df_phi4])\n",
    "df_responses['model_response_len']  = df_responses['model_response'].str.len()\n",
    "df_responses['python']              = df_responses['model_response'].apply(lambda x: extractPythonBlock(x))\n",
    "df_responses['python_exists']       = df_responses['python'].apply(lambda x: x is not None)\n",
    "rt.tile([rt.histogram(df_responses, bin_by='model', count_by='time_taken',         color_by='model', w=256, h=64),\n",
    "         rt.histogram(df_responses, bin_by='model', count_by='model_response_len', color_by='model', w=256, h=64),\n",
    "         rt.histogram(df_responses, bin_by='model',                                color_by='python_exists', w=256, h=64)], spacer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "expression = \"{'a': 1, 'b': 2}\"\n",
    "result = ast.literal_eval(expression)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_responses)):\n",
    "    if df_responses.iloc[i]['python'] is None: continue\n",
    "    _str_ = df_responses.iloc[i]['python']\n",
    "    if 'entities ='      in _str_: _str_ = _str_.replace('entities =', '')\n",
    "    if 'print(entities)' in _str_: _str_ = _str_.replace('print(entities)', '')\n",
    "    try:\n",
    "        ast.literal_eval(_str_)\n",
    "    except:\n",
    "        print('---')\n",
    "        print(_str_)\n",
    "        print('...')\n",
    "        print(df_responses.iloc[i]['python'])\n",
    "        print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast.literal_eval('''{\n",
    "    \"1545\": \"time\",\n",
    "    \"dark clothed figures\": \"group_description\",\n",
    "    \"event robbing\": \"activity\",\n",
    "    \"victims\": \"people_group\",\n",
    "    \"GAStech\": \"organization\",\n",
    "    \"evacuation of the construction site\": \"event\",\n",
    "    \"confusion\": \"situation\",\n",
    "}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=223112.32313\n",
    "print(f'{x:,.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
