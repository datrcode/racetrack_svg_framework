{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "pipeline = transformers.pipeline(\"text-generation\",model=model_id,model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\",)\n",
    "def promptModel(_user_, _system_='You are a helpful digital assistant.', max_tokens=256):\n",
    "    messages = [{\"role\": \"system\", \"content\": _system_},{\"role\": \"user\",   \"content\": _user_}]\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    terminators = [pipeline.tokenizer.eos_token_id,pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "    outputs = pipeline(prompt,max_new_tokens=max_tokens,eos_token_id=terminators,\n",
    "                       do_sample=True,temperature=0.6,top_p=0.9,)\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Prepare an article for processing by loading it, separating it into sentences, and doing basic cleaning\n",
    "#\n",
    "import os\n",
    "_base_dir_ = '../../../data/2014_vast/MC1/News Articles'\n",
    "os.listdir(_base_dir_ + '/Everyday News')\n",
    "_txt_raw_     = open(_base_dir_ + '/Everyday News/343.txt', 'rb').read()\n",
    "_txt_better_  = str(_txt_raw_, encoding='utf-8').replace('\\r', '').split('\\n')\n",
    "_txt_         = []\n",
    "for _line_ in _txt_better_:\n",
    "    if _line_ == '' or _line_ == '\\n' or _line_ == '\\r': continue\n",
    "    if '.' in _line_:\n",
    "        for _sent_ in _line_.split('.'):\n",
    "            _sent_ = _sent_.strip()\n",
    "            if _sent_ == '' or _sent_ == '\\n' or _sent_ == '\\r' or _sent_ == '.': continue\n",
    "            _txt_.append(_sent_ + '.')\n",
    "    else:\n",
    "        _txt_.append(_line_.strip())\n",
    "print('\\n'.join(_txt_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Grammar is a problem with the articles -- let's have the model fix that up first\n",
    "#\n",
    "_grammar_ = []\n",
    "for _sent_ in _txt_:\n",
    "    if _sent_.lower().startswith('source:') or _sent_.lower().startswith('title:') or _sent_.lower().startswith('published:'): continue\n",
    "    _fixed_up_ = promptModel(_sent_, 'Make the following sentence grammatically correct.  Just return the corrected sentence with no explanation.')    \n",
    "    _grammar_.append(_fixed_up_)\n",
    "print('\\n'.join(_grammar_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "_entity_prompt_ = '''Extract the people, groups, organizations, countries, cities, places, and locations from the following sentence.\n",
    "Return as CSV with a header of \"entity, type\".  Just return the CSV.  Do not include an explanation or any caveats.'''\n",
    "i = 7\n",
    "_example_ = promptModel(_grammar_[i], _entity_prompt_)\n",
    "print(_grammar_[i])\n",
    "print(_example_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
