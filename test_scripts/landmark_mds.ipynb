{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e840a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Claude response for the following:  Create python code for the Landmark MDS algorithm.  The code should accept a single argument, g, which is a graph.\n",
    "# ... and then several iterations of changes to make it work...\n",
    "#\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class LandmarkMDSLayout(object):\n",
    "    def __init__(self, g, num_landmarks=None, dimensions=2):\n",
    "        \"\"\"\n",
    "        Landmark Multidimensional Scaling (L-MDS) algorithm for graph embedding.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        g : networkx.Graph or scipy.sparse matrix or numpy.ndarray\n",
    "            Input graph. Can be a NetworkX graph, sparse matrix, or dense adjacency matrix.\n",
    "        num_landmarks : int, optional\n",
    "            Number of landmark nodes to select. If None, uses sqrt(n) where n is number of nodes.\n",
    "        dimensions : int, default=2\n",
    "            Number of dimensions for the embedding.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert graph to adjacency matrix if needed\n",
    "        if isinstance(g, nx.Graph):\n",
    "            n = g.number_of_nodes()\n",
    "            # Create adjacency matrix with edge weights\n",
    "            adj_matrix = nx.to_scipy_sparse_array(g, weight='weight', format='csr')\n",
    "        elif isinstance(g, csr_matrix):\n",
    "            adj_matrix = g\n",
    "            n = adj_matrix.shape[0]\n",
    "        elif isinstance(g, np.ndarray):\n",
    "            adj_matrix = csr_matrix(g)\n",
    "            n = adj_matrix.shape[0]\n",
    "        else:\n",
    "            raise ValueError(\"Graph must be NetworkX graph, scipy sparse matrix, or numpy array\")\n",
    "        \n",
    "        # Set number of landmarks if not specified\n",
    "        if num_landmarks is None:\n",
    "            num_landmarks = max(int(np.sqrt(n)), dimensions + 1)\n",
    "        \n",
    "        num_landmarks = min(num_landmarks, n)\n",
    "        \n",
    "        # Step 1: Select landmark nodes (random selection)\n",
    "        landmarks = np.random.choice(n, size=num_landmarks, replace=False)\n",
    "        \n",
    "        # Step 2: Compute shortest path distances from all nodes to landmarks\n",
    "        # Using Dijkstra's algorithm from each landmark\n",
    "        distances = np.zeros((n, num_landmarks))\n",
    "        \n",
    "        for i, landmark in enumerate(landmarks):\n",
    "            dist = dijkstra(adj_matrix, directed=False, indices=landmark)\n",
    "            distances[:, i] = dist\n",
    "        \n",
    "        # Handle infinite distances (disconnected components)\n",
    "        max_finite_dist = np.max(distances[np.isfinite(distances)])\n",
    "        distances[np.isinf(distances)] = 2 * max_finite_dist\n",
    "        \n",
    "        # Step 3: Apply classical MDS on the distance matrix\n",
    "        # Center the squared distance matrix\n",
    "        D_squared = distances ** 2\n",
    "        n_samples = D_squared.shape[0]\n",
    "        n_landmarks = D_squared.shape[1]\n",
    "        \n",
    "        # Centering matrix\n",
    "        landmark_mean = D_squared.mean(axis=0)\n",
    "        overall_mean = D_squared.mean()\n",
    "        \n",
    "        # Double centering\n",
    "        B = -0.5 * (D_squared - landmark_mean - D_squared.mean(axis=1, keepdims=True) + overall_mean)\n",
    "        \n",
    "        # Step 4: Compute eigendecomposition and extract coordinates\n",
    "        # Use PCA for efficiency (equivalent to eigendecomposition)\n",
    "        pca = PCA(n_components=dimensions)\n",
    "        coords = pca.fit_transform(B)\n",
    "        \n",
    "        # Create node mapping for NetworkX graphs\n",
    "        node_mapping = None\n",
    "        if isinstance(g, nx.Graph):\n",
    "            node_mapping = list(g.nodes())\n",
    "            \n",
    "        # Get landmark coordinates\n",
    "        landmark_coords = coords[landmarks]\n",
    "        \n",
    "        # Refine non-landmark positions using weighted least squares\n",
    "        # (triangulation based on distances to landmarks)\n",
    "        if isinstance(g, nx.Graph):\n",
    "            adj_matrix = nx.to_scipy_sparse_array(g, weight='weight', format='csr')\n",
    "        elif isinstance(g, csr_matrix):\n",
    "            adj_matrix = g\n",
    "        else:\n",
    "            adj_matrix = csr_matrix(g)\n",
    "        \n",
    "        n = adj_matrix.shape[0]\n",
    "        \n",
    "        # Compute distances to landmarks again\n",
    "        distances = np.zeros((n, len(landmarks)))\n",
    "        for i, landmark in enumerate(landmarks):\n",
    "            dist = dijkstra(adj_matrix, directed=False, indices=landmark)\n",
    "            distances[:, i] = dist\n",
    "        \n",
    "        max_finite_dist = np.max(distances[np.isfinite(distances)])\n",
    "        distances[np.isinf(distances)] = 2 * max_finite_dist\n",
    "\n",
    "        self.coords, self.landmarks, self.node_mapping = coords, landmarks, node_mapping\n",
    "    \n",
    "    def results(self):\n",
    "        _pos_ = {}\n",
    "        for i in range(len(self.coords)): _pos_[self.node_mapping[i]] = self.coords[i]\n",
    "        return _pos_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4203bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import rtsvg\n",
    "rt = rtsvg.RACETrack()\n",
    "from linknode_graph_patterns import  LinkNodeGraphPatterns\n",
    "_patterns_ = LinkNodeGraphPatterns()\n",
    "_g_        = _patterns_.createPattern('stanford_facebook_networks')\n",
    "_df_       = _patterns_.nxGraphToPolarsDataFrame(_g_)\n",
    "\n",
    "# Create the layout & fill in the pos dictionary\n",
    "t0 = time.time()\n",
    "_pos_lmds_ = LandmarkMDSLayout(_g_).results()\n",
    "t1 = time.time()\n",
    "_pos_pfdl_ = rtsvg.PolarsForceDirectedLayout(_g_).results()\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"LMDs: {(t1-t0):.2f} | PFDL: {(t2-t1):.2f}\") # LMDs: 0.17 | PFDL: 16.10\n",
    "\n",
    "# Color the nodes\n",
    "_node_colors_ = {}\n",
    "community_i   = 0\n",
    "for _community_ in nx.community.louvain_communities(_g_):\n",
    "    community_i += 1\n",
    "    for _node_ in _community_: _node_colors_[_node_] = rt.co_mgr.getColor(community_i)\n",
    "\n",
    "# Display it\n",
    "_params_ = {'df':_df_, 'relationships':[('fm','to')], 'node_color':_node_colors_, 'w':512, 'h':512}\n",
    "rt.tile([rt.link(pos=_pos_lmds_, **_params_), rt.link(pos=_pos_pfdl_, **_params_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Then with Claude: \"In a similar fashion, write the code for Pivot MDS\"\n",
    "#\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def pivot_mds(g, num_pivots=None, dimensions=2):\n",
    "    \"\"\"\n",
    "    Pivot Multidimensional Scaling (Pivot MDS) algorithm for graph embedding.\n",
    "    \n",
    "    Pivot MDS uses a subset of nodes (pivots) to compute a low-dimensional embedding\n",
    "    by treating the distances to pivots as coordinates directly, then reducing dimensions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    g : networkx.Graph or scipy.sparse matrix or numpy.ndarray\n",
    "        Input graph. Can be a NetworkX graph, sparse matrix, or dense adjacency matrix.\n",
    "    num_pivots : int, optional\n",
    "        Number of pivot nodes to select. If None, uses sqrt(n) where n is number of nodes.\n",
    "    dimensions : int, default=2\n",
    "        Number of dimensions for the final embedding.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    coords : numpy.ndarray\n",
    "        Coordinates of all nodes in the embedded space (n_nodes x dimensions).\n",
    "    pivots : numpy.ndarray\n",
    "        Indices of selected pivot nodes.\n",
    "    node_mapping : list or None\n",
    "        List mapping array indices to original node identifiers (only for NetworkX graphs).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert graph to adjacency matrix if needed\n",
    "    if isinstance(g, nx.Graph):\n",
    "        n = g.number_of_nodes()\n",
    "        node_mapping = list(g.nodes())\n",
    "        # Create adjacency matrix with edge weights\n",
    "        adj_matrix = nx.to_scipy_sparse_array(g, weight='weight', format='csr')\n",
    "    elif isinstance(g, csr_matrix):\n",
    "        adj_matrix = g\n",
    "        n = adj_matrix.shape[0]\n",
    "        node_mapping = None\n",
    "    elif isinstance(g, np.ndarray):\n",
    "        adj_matrix = csr_matrix(g)\n",
    "        n = adj_matrix.shape[0]\n",
    "        node_mapping = None\n",
    "    else:\n",
    "        raise ValueError(\"Graph must be NetworkX graph, scipy sparse matrix, or numpy array\")\n",
    "    \n",
    "    # Set number of pivots if not specified\n",
    "    if num_pivots is None:\n",
    "        num_pivots = max(int(np.sqrt(n)), dimensions + 1)\n",
    "    \n",
    "    num_pivots = min(num_pivots, n)\n",
    "    \n",
    "    # Step 1: Select pivot nodes (random selection)\n",
    "    pivots = np.random.choice(n, size=num_pivots, replace=False)\n",
    "    \n",
    "    # Step 2: Compute shortest path distances from all nodes to pivots\n",
    "    distances = np.zeros((n, num_pivots))\n",
    "    \n",
    "    for i, pivot in enumerate(pivots):\n",
    "        dist = dijkstra(adj_matrix, directed=False, indices=pivot)\n",
    "        distances[:, i] = dist\n",
    "    \n",
    "    # Handle infinite distances (disconnected components)\n",
    "    max_finite_dist = np.max(distances[np.isfinite(distances)])\n",
    "    distances[np.isinf(distances)] = 2 * max_finite_dist\n",
    "    \n",
    "    # Step 3: Center the distance matrix\n",
    "    # Subtract mean distance to each pivot\n",
    "    col_means = distances.mean(axis=0)\n",
    "    distances_centered = distances - col_means\n",
    "    \n",
    "    # Step 4: Dimensionality reduction using SVD\n",
    "    # The centered distance matrix is our feature representation\n",
    "    if num_pivots > dimensions:\n",
    "        # Use SVD to reduce to desired dimensions\n",
    "        U, S, Vt = np.linalg.svd(distances_centered, full_matrices=False)\n",
    "        \n",
    "        # Take first 'dimensions' components\n",
    "        coords = U[:, :dimensions] * S[:dimensions]\n",
    "    else:\n",
    "        # If we have fewer pivots than desired dimensions, use all\n",
    "        coords = distances_centered\n",
    "    \n",
    "    return coords, pivots, node_mapping\n",
    "\n",
    "\n",
    "def pivot_mds_maxmin(g, num_pivots=None, dimensions=2):\n",
    "    \"\"\"\n",
    "    Pivot MDS with MaxMin pivot selection strategy.\n",
    "    \n",
    "    Instead of random selection, pivots are chosen iteratively to maximize\n",
    "    the minimum distance to already selected pivots, providing better coverage.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    g : networkx.Graph or scipy.sparse matrix or numpy.ndarray\n",
    "        Input graph.\n",
    "    num_pivots : int, optional\n",
    "        Number of pivot nodes to select.\n",
    "    dimensions : int, default=2\n",
    "        Number of dimensions for the final embedding.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    coords : numpy.ndarray\n",
    "        Coordinates of all nodes in the embedded space.\n",
    "    pivots : numpy.ndarray\n",
    "        Indices of selected pivot nodes.\n",
    "    node_mapping : list or None\n",
    "        List mapping array indices to original node identifiers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert graph to adjacency matrix if needed\n",
    "    if isinstance(g, nx.Graph):\n",
    "        n = g.number_of_nodes()\n",
    "        node_mapping = list(g.nodes())\n",
    "        adj_matrix = nx.to_scipy_sparse_array(g, weight='weight', format='csr')\n",
    "    elif isinstance(g, csr_matrix):\n",
    "        adj_matrix = g\n",
    "        n = adj_matrix.shape[0]\n",
    "        node_mapping = None\n",
    "    elif isinstance(g, np.ndarray):\n",
    "        adj_matrix = csr_matrix(g)\n",
    "        n = adj_matrix.shape[0]\n",
    "        node_mapping = None\n",
    "    else:\n",
    "        raise ValueError(\"Graph must be NetworkX graph, scipy sparse matrix, or numpy array\")\n",
    "    \n",
    "    # Set number of pivots if not specified\n",
    "    if num_pivots is None:\n",
    "        num_pivots = max(int(np.sqrt(n)), dimensions + 1)\n",
    "    \n",
    "    num_pivots = min(num_pivots, n)\n",
    "    \n",
    "    # Step 1: Select pivots using MaxMin strategy\n",
    "    pivots = []\n",
    "    \n",
    "    # Select first pivot randomly\n",
    "    first_pivot = np.random.randint(0, n)\n",
    "    pivots.append(first_pivot)\n",
    "    \n",
    "    # Initialize minimum distances to first pivot\n",
    "    min_distances = dijkstra(adj_matrix, directed=False, indices=first_pivot)\n",
    "    min_distances[np.isinf(min_distances)] = 0\n",
    "    \n",
    "    # Select remaining pivots\n",
    "    for _ in range(num_pivots - 1):\n",
    "        # Select node with maximum minimum distance to existing pivots\n",
    "        next_pivot = np.argmax(min_distances)\n",
    "        pivots.append(next_pivot)\n",
    "        \n",
    "        # Update minimum distances\n",
    "        new_distances = dijkstra(adj_matrix, directed=False, indices=next_pivot)\n",
    "        new_distances[np.isinf(new_distances)] = 0\n",
    "        min_distances = np.minimum(min_distances, new_distances)\n",
    "    \n",
    "    pivots = np.array(pivots)\n",
    "    \n",
    "    # Step 2: Compute distances from all nodes to selected pivots\n",
    "    distances = np.zeros((n, num_pivots))\n",
    "    \n",
    "    for i, pivot in enumerate(pivots):\n",
    "        dist = dijkstra(adj_matrix, directed=False, indices=pivot)\n",
    "        distances[:, i] = dist\n",
    "    \n",
    "    # Handle infinite distances\n",
    "    max_finite_dist = np.max(distances[np.isfinite(distances)])\n",
    "    distances[np.isinf(distances)] = 2 * max_finite_dist\n",
    "    \n",
    "    # Step 3: Center and reduce dimensions\n",
    "    col_means = distances.mean(axis=0)\n",
    "    distances_centered = distances - col_means\n",
    "    \n",
    "    # Step 4: SVD for dimensionality reduction\n",
    "    if num_pivots > dimensions:\n",
    "        U, S, Vt = np.linalg.svd(distances_centered, full_matrices=False)\n",
    "        coords = U[:, :dimensions] * S[:dimensions]\n",
    "    else:\n",
    "        coords = distances_centered\n",
    "    \n",
    "    return coords, pivots, node_mapping\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a sample graph\n",
    "    G = nx.karate_club_graph()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Pivot MDS with Random Selection\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Apply Pivot MDS with random pivot selection\n",
    "    coordinates, pivot_indices, node_mapping = pivot_mds(G, num_pivots=10, dimensions=2)\n",
    "    \n",
    "    print(f\"Graph has {G.number_of_nodes()} nodes\")\n",
    "    print(f\"Selected {len(pivot_indices)} pivot indices: {pivot_indices}\")\n",
    "    \n",
    "    # Get original node identifiers for pivots\n",
    "    pivot_nodes = [node_mapping[i] for i in pivot_indices]\n",
    "    print(f\"Pivot node IDs: {pivot_nodes}\")\n",
    "    \n",
    "    # Get non-pivot information\n",
    "    non_pivot_indices = np.setdiff1d(np.arange(len(coordinates)), pivot_indices)\n",
    "    non_pivot_nodes = [node_mapping[i] for i in non_pivot_indices]\n",
    "    \n",
    "    print(f\"\\nFirst 5 non-pivot nodes:\")\n",
    "    for i in range(min(5, len(non_pivot_indices))):\n",
    "        idx = non_pivot_indices[i]\n",
    "        node_id = node_mapping[idx]\n",
    "        coord = coordinates[idx]\n",
    "        print(f\"  Node {node_id}: {coord}\")\n",
    "    \n",
    "    print(f\"\\nEmbedding shape: {coordinates.shape}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Pivot MDS with MaxMin Selection\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Apply Pivot MDS with MaxMin pivot selection\n",
    "    coordinates_mm, pivot_indices_mm, node_mapping_mm = pivot_mds_maxmin(G, num_pivots=10, dimensions=2)\n",
    "    \n",
    "    print(f\"Selected {len(pivot_indices_mm)} pivot indices: {pivot_indices_mm}\")\n",
    "    pivot_nodes_mm = [node_mapping_mm[i] for i in pivot_indices_mm]\n",
    "    print(f\"Pivot node IDs (MaxMin): {pivot_nodes_mm}\")\n",
    "    \n",
    "    print(f\"\\nEmbedding shape: {coordinates_mm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b3965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
