{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71406afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars   as pl\n",
    "import networkx as nx\n",
    "pl.Config(tbl_width_chars=500)\n",
    "pl.Config(tbl_cols=-1)\n",
    "import rtsvg\n",
    "rt = rtsvg.RACETrack()\n",
    "from  polars_spring_layout    import PolarsSpringLayout\n",
    "from  linknode_graph_patterns import LinkNodeGraphPatterns\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "'''\n",
    "edges_filename  = '../../data/stanford/facebook/348.edges'\n",
    "_lu_  = {'fm':[], 'to':[]}\n",
    "_pos_ = {}\n",
    "for _edge_ in open(edges_filename, 'rt').read().split('\\n'):\n",
    "    if _edge_ == '': continue\n",
    "    _split_     = _edge_.split()\n",
    "    _fm_, _to_  = int(_split_[0]), int(_split_[1])\n",
    "    _lu_['fm'].append(_fm_), _lu_['to'].append(_to_)\n",
    "    _pos_[_fm_] = [10.0+random.random()*100.0, 20.0+random.random()*100.0]\n",
    "    _pos_[_to_] = [10.0+random.random()*100.0, 20.0+random.random()*100.0]\n",
    "'''\n",
    "_lu_  = {'fm':'a b c a b c c c c c c c'.split(), 'to':'b c a a0 b0 c0 c1 c2 c3 c4 c5 c6'.split()}\n",
    "_df_  = pl.DataFrame(_lu_)\n",
    "_g_   = rt.createNetworkXGraph(_df_, [('fm','to')])\n",
    "_pos_ = {}\n",
    "for _node_ in _g_.nodes(): _pos_[_node_] = [random.random(), random.random()]\n",
    "_iterations_ = None\n",
    "t0 = time.time()\n",
    "_pos_ref_ = rt.springLayout   (_g_, copy.deepcopy(_pos_), iterations=_iterations_)\n",
    "t1 = time.time()\n",
    "_psl_     = PolarsSpringLayout(_g_, copy.deepcopy(_pos_), iterations=_iterations_)\n",
    "_pos_     = _psl_.results()\n",
    "t2 = time.time()\n",
    "_pos_ref2_ = nx.spring_layout(_g_)\n",
    "t3 = time.time()\n",
    "'''\n",
    "rt.tile([rt.link(_df_, [('fm','to')], _pos_ref_), \n",
    "         rt.link(_df_, [('fm','to')], _pos_ref2_), \n",
    "         rt.link(_df_, [('fm','to')], _pos_)], spacer=10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa30b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref Time = 5.1s | Polars Time = 0.8s | M1 Pro @ 16GB\n",
    "# Ref Time = 5.14s, Polars Time = 0.82s, NX Ref Time = 0.06s | M1 Pro @ 16GB\n",
    "# Ref Time = 5.15s, Polars Time = 0.79s, NX Ref Time = 0.05s | M1 Pro @ 16GB // removed dx and dy tables from polars impl\n",
    "# Ref Time = 5.16s, Polars Time = 0.83s, NX Ref Time = 0.05s | M1 Pro @ 16GB // with the static nodes requirement\n",
    "# \n",
    "print(f'Ref Time = {t1-t0:.2f}s, Polars Time = {t2-t1:.2f}s, NX Ref Time = {t3-t2:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144826d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphToDataFrame(_g_):\n",
    "    _lu_ = {'fm':[], 'to':[]}\n",
    "    for _node_ in _g_.nodes(): \n",
    "        for _nbor_ in _g_.neighbors(_node_):\n",
    "            _lu_['fm'].append(_node_), _lu_['to'].append(_nbor_)\n",
    "    return pl.DataFrame(_lu_), [('fm','to')]\n",
    "\n",
    "_patterns_ = LinkNodeGraphPatterns()\n",
    "_tiles_    = []\n",
    "for _type_ in _patterns_.types:\n",
    "    _g_             = _patterns_.createPattern(_type_)\n",
    "    _df_, _relates_ = graphToDataFrame(_g_)\n",
    "    _pos1_ = nx.spring_layout(_g_)\n",
    "    _pos2_ = rt.springLayout(_g_)\n",
    "    _pos3_ = PolarsSpringLayout(_g_).results()\n",
    "    _tiles_.append(rt.link(_df_, _relates_, _pos1_))\n",
    "    _tiles_.append(rt.link(_df_, _relates_, _pos2_))\n",
    "    _tiles_.append(rt.link(_df_, _relates_, _pos3_))\n",
    "#rt.table(_tiles_, per_row=3, spacer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacc58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyLandmarks(_g_, num_landmarks=8):\n",
    "    mins, sums = {}, {}\n",
    "    for node in _g_.nodes(): mins[node], sums[node] = 1e10, 0.0\n",
    "    # Pick a random seed, calculate the single source dijkstra, and find the largest distance node -- that's the first landmark\n",
    "    _seed_  = random.choice(list(_g_.nodes()))\n",
    "    _ssd_   = nx.single_source_dijkstra(_g_, _seed_)\n",
    "    _max_   = None\n",
    "    for _node_, _dist_ in _ssd_[0].items():\n",
    "        if _node_ == _seed_: continue\n",
    "        if _max_ is None or _dist_ > _max_[1]: _max_ = (_node_, _dist_)\n",
    "    # Construct the data structures to find the landmarks\n",
    "    next_node  = _max_[0]\n",
    "    found      = set([_max_[0]])\n",
    "    shortests  = {}\n",
    "    if num_landmarks == 1: return found\n",
    "\n",
    "    while len(found) < num_landmarks and len(found) < len(_g_.nodes()):\n",
    "        shortest = nx.single_source_dijkstra(_g_, next_node)\n",
    "        shortests[next_node] = shortest\n",
    "        max_sum, max_sum_node, max_sum_min = -1e10, None, 1e10\n",
    "        # Iterate over the nodes updating the sums/mins\n",
    "        for node in _g_.nodes():\n",
    "            d = shortest[0][node]\n",
    "            # Update mins and sums\n",
    "            if mins[node] > d: mins[node] = d\n",
    "            sums[node] = sums[node] + d\n",
    "            if sums[node] > max_sum and node not in found: \n",
    "                max_sums, max_sum_node, max_sum_min = sums[node], node, mins[node]\n",
    "        # Find all the nodes that have that sum\n",
    "        max_sum_set = set([max_sum_node])\n",
    "        for node in _g_.nodes():\n",
    "            if sums[node] >= max_sum: max_sum_set.add(node)\n",
    "        # If only one, then that's the next node... otherwise, choose the one with the highest min\n",
    "        if len(max_sum_set) == 1:\n",
    "            next_node = max_sum_set.pop()\n",
    "        else:\n",
    "            for node in max_sum_set:\n",
    "                if mins[node] > max_sum_min:\n",
    "                    max_sum_node = node\n",
    "                    max_sum_min  = mins[node]\n",
    "            next_node = max_sum_node\n",
    "        found.add(next_node)\n",
    "    return found\n",
    "\n",
    "_pos_ = PolarsSpringLayout(_g_).results()\n",
    "_tiles_ = []\n",
    "for i in range(10):\n",
    "    _node_colors_ = {}\n",
    "    for _landmark_ in identifyLandmarks(_g_, 3): _node_colors_[_landmark_] = 'red'\n",
    "    _tiles_.append(rt.link(_df_, _relates_, _pos_, node_color=_node_colors_))\n",
    "rt.table(_tiles_, per_row=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d5ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
