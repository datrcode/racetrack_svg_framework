{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71406afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars   as pl\n",
    "import networkx as nx\n",
    "pl.Config(tbl_width_chars=500)\n",
    "pl.Config(tbl_cols=-1)\n",
    "import rtsvg\n",
    "rt = rtsvg.RACETrack()\n",
    "from  polars_spring_layout    import PolarsSpringLayout\n",
    "from  linknode_graph_patterns import LinkNodeGraphPatterns\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "\n",
    "edges_filename  = '../../data/stanford/facebook/348.edges'\n",
    "_lu_  = {'fm':[], 'to':[]}\n",
    "_pos_ = {}\n",
    "for _edge_ in open(edges_filename, 'rt').read().split('\\n'):\n",
    "    if _edge_ == '': continue\n",
    "    _split_     = _edge_.split()\n",
    "    _fm_, _to_  = int(_split_[0]), int(_split_[1])\n",
    "    _lu_['fm'].append(_fm_), _lu_['to'].append(_to_)\n",
    "    _pos_[_fm_] = [10.0+random.random()*100.0, 20.0+random.random()*100.0]\n",
    "    _pos_[_to_] = [10.0+random.random()*100.0, 20.0+random.random()*100.0]\n",
    "\n",
    "#_lu_  = {'fm':'a b c a b c c c c c c c'.split(), 'to':'b c a a0 b0 c0 c1 c2 c3 c4 c5 c6'.split()}\n",
    "_df_  = pl.DataFrame(_lu_)\n",
    "_g_   = rt.createNetworkXGraph(_df_, [('fm','to')])\n",
    "_pos_ = {}\n",
    "for _node_ in _g_.nodes(): _pos_[_node_] = [random.random(), random.random()]\n",
    "_iterations_ = None\n",
    "t0 = time.time()\n",
    "_pos_ref_ = rt.springLayout   (_g_, copy.deepcopy(_pos_), iterations=_iterations_)\n",
    "t1 = time.time()\n",
    "_psl_     = PolarsSpringLayout(_g_, copy.deepcopy(_pos_), iterations=_iterations_)\n",
    "_pos_     = _psl_.results()\n",
    "t2 = time.time()\n",
    "_pos_ref2_ = nx.spring_layout(_g_)\n",
    "t3 = time.time()\n",
    "rt.tile([rt.link(_df_, [('fm','to')], _pos_ref_), \n",
    "         rt.link(_df_, [('fm','to')], _pos_ref2_), \n",
    "         rt.link(_df_, [('fm','to')], _pos_)], spacer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa30b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref Time = 5.1s | Polars Time = 0.8s | M1 Pro @ 16GB\n",
    "# Ref Time = 5.14s, Polars Time = 0.82s, NX Ref Time = 0.06s | M1 Pro @ 16GB\n",
    "# Ref Time = 5.15s, Polars Time = 0.79s, NX Ref Time = 0.05s | M1 Pro @ 16GB // removed dx and dy tables from polars impl\n",
    "# Ref Time = 5.16s, Polars Time = 0.83s, NX Ref Time = 0.05s | M1 Pro @ 16GB // with the static nodes requirement\n",
    "# \n",
    "print(f'Ref Time = {t1-t0:.2f}s, Polars Time = {t2-t1:.2f}s, NX Ref Time = {t3-t2:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144826d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphToDataFrame(_g_):\n",
    "    _lu_ = {'fm':[], 'to':[]}\n",
    "    for _node_ in _g_.nodes(): \n",
    "        for _nbor_ in _g_.neighbors(_node_):\n",
    "            _lu_['fm'].append(_node_), _lu_['to'].append(_nbor_)\n",
    "    return pl.DataFrame(_lu_), [('fm','to')]\n",
    "\n",
    "_patterns_ = LinkNodeGraphPatterns()\n",
    "_tiles_    = []\n",
    "for _type_ in _patterns_.types:\n",
    "    _g_             = _patterns_.createPattern(_type_)\n",
    "    _df_, _relates_ = graphToDataFrame(_g_)\n",
    "    _pos1_ = nx.spring_layout(_g_)\n",
    "    _pos2_ = rt.springLayout(_g_)\n",
    "    _pos3_ = PolarsSpringLayout(_g_).results()\n",
    "    _tiles_.append(rt.link(_df_, _relates_, _pos1_))\n",
    "    _tiles_.append(rt.link(_df_, _relates_, _pos2_))\n",
    "    _tiles_.append(rt.link(_df_, _relates_, _pos3_))\n",
    "#rt.table(_tiles_, per_row=3, spacer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf577c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Prototyping Landmark Detection\n",
    "#\n",
    "_g_             = _patterns_.createPattern('mesh')\n",
    "_df_, _relates_ = graphToDataFrame(_g_)\n",
    "_d_   = {}\n",
    "_gen_ = dict(nx.all_pairs_shortest_path_length(_g_))\n",
    "_lu_  = {'fm':[],'to':[], 't':[]}\n",
    "for _node_ in _gen_.keys():\n",
    "    for _nbor_ in _gen_[_node_].keys():\n",
    "        if _node_ == _nbor_: continue\n",
    "        _lu_['fm'].append(_node_), _lu_['to'].append(_nbor_), _lu_['t'].append(_gen_[_node_][_nbor_])\n",
    "_df_dists_  = pl.DataFrame(_lu_)\n",
    "_landmarks_ = set()\n",
    "_landmarks_.add(_df_dists_.sample(1)['fm'][0])\n",
    "\n",
    "for i in range(5):\n",
    "    _fm_landmarks_  = _df_dists_.filter(pl.col('fm').is_in(_landmarks_) & ~pl.col('to').is_in(_landmarks_))\n",
    "    _fm_sums_       = _fm_landmarks_.group_by('to').agg(pl.col('t').sum().alias('t_sum'))\n",
    "    _largest_index_ = _fm_sums_['t_sum'].arg_max()\n",
    "    _landmarks_.add(_fm_sums_['to'][_largest_index_])\n",
    "\n",
    "_node_colors_ = {}\n",
    "for _landmark_ in _landmarks_: _node_colors_[_landmark_] = 'red'\n",
    "\n",
    "_pos_ = PolarsSpringLayout(_g_).results()\n",
    "rt.link(_df_, _relates_, _pos_, node_color=_node_colors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cd081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyLandmarks(_g_, num_landmarks = 8):\n",
    "    # Pick a random seed, calculate the single source dijkstra, and find the largest distance node -- that's the first landmark\n",
    "    _seed_  = random.choice(list(_g_.nodes()))\n",
    "    _ssd_   = nx.single_source_dijkstra(_g_, _seed_)\n",
    "    _max_   = None\n",
    "    for _node_, _dist_ in _ssd_[0].items():\n",
    "        if _node_ == _seed_: continue\n",
    "        if _max_ is None or _dist_ > _max_[1]: _max_ = (_node_, _dist_)\n",
    "    # Construct the two data structures to find the landmarks\n",
    "    _lms_   = set([_max_[0]])\n",
    "    _ssds_  = {_seed_: _ssd_} # in the random case where the seed should have been a landmark\n",
    "    if num_landmarks == 1: return _lms_\n",
    "\n",
    "    # Until we have enough landmarks ...\n",
    "    while len(_lms_) < num_landmarks:\n",
    "        # Find the max for all the landmarks (that's not actually a landmark too)\n",
    "        _maxes_ = []\n",
    "        for _lm_ in _lms_:\n",
    "            _max_ = None\n",
    "            if _lm_ not in _ssds_: _ssds_[_lm_] = nx.single_source_dijkstra(_g_, _lm_)\n",
    "            for _node_, _dist_ in _ssds_[_lm_][0].items():\n",
    "                if _node_ in _lms_: continue\n",
    "                if _max_ is None or _dist_ > _max_[1]: _max_ = (_node_, _dist_, _lm_)\n",
    "            _maxes_.append(_max_)\n",
    "        # If we only have one landmark (one max), then that  one is the landmark for this iteration\n",
    "        if len(_maxes_) == 1:\n",
    "            _lms_.add(_maxes_[0][0])\n",
    "        else:\n",
    "            # Else, we want to find the one with the highest *min* distance to all the landmarks\n",
    "            _mins_ = []\n",
    "            for _max_ in _maxes_:\n",
    "                _distances_to_possible_ = []\n",
    "                _possible_ = _max_[0]\n",
    "                _from_     = _max_[2]\n",
    "                for _lm_ in _lms_:\n",
    "                    if _from_ == _lm_: continue\n",
    "                    _distances_to_possible_.append(_ssds_[_lm_][0][_possible_])\n",
    "                _min_found_ = min(_distances_to_possible_)\n",
    "                _mins_.append((_possible_, _min_found_))\n",
    "            _best_ = _mins_[0]\n",
    "            for _min_ in _mins_:\n",
    "                if _min_[1] < _best_[1]: _best_ = _min_\n",
    "            _lms_.add(_best_[0])\n",
    "\n",
    "    return _lms_\n",
    "\n",
    "\n",
    "_node_colors_ = {}\n",
    "for _landmark_ in identifyLandmarks(_g_, 8): _node_colors_[_landmark_] = 'red'\n",
    "\n",
    "_pos_ = PolarsSpringLayout(_g_).results()\n",
    "rt.link(_df_, _relates_, _pos_, node_color=_node_colors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacc58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyLandmarks(_g_, num_landmarks=8):\n",
    "    mins, sums = {}, {}\n",
    "    for node in _g_.nodes(): mins[node], sums[node] = 1e10, 0.0\n",
    "    # Pick a random seed, calculate the single source dijkstra, and find the largest distance node -- that's the first landmark\n",
    "    _seed_  = random.choice(list(_g_.nodes()))\n",
    "    _ssd_   = nx.single_source_dijkstra(_g_, _seed_)\n",
    "    _max_   = None\n",
    "    for _node_, _dist_ in _ssd_[0].items():\n",
    "        if _node_ == _seed_: continue\n",
    "        if _max_ is None or _dist_ > _max_[1]: _max_ = (_node_, _dist_)\n",
    "    # Construct the data structures to find the landmarks\n",
    "    next_node  = _max_[0]\n",
    "    found      = set([_max_[0]])\n",
    "    shortests  = {}\n",
    "    if num_landmarks == 1: return found\n",
    "\n",
    "    while len(found) < num_landmarks and len(found) < len(_g_.nodes()):\n",
    "        shortest = nx.single_source_dijkstra(_g_, next_node)\n",
    "        shortests[next_node] = shortest\n",
    "        max_sum, max_sum_node, max_sum_min = -1e10, None, 1e10\n",
    "        # Iterate over the nodes updating the sums/mins\n",
    "        for node in _g_.nodes():\n",
    "            d = shortest[0][node]\n",
    "            # Update mins and sums\n",
    "            if mins[node] > d: mins[node] = d\n",
    "            sums[node] = sums[node] + d\n",
    "            if sums[node] > max_sum and node not in found:\n",
    "                max_sums, max_sum_node, max_sum_min = sums[node], node, mins[node]\n",
    "        # Find all the nodes that have that sum\n",
    "        max_sum_set = set([max_sum_node])\n",
    "        for node in _g_.nodes():\n",
    "            if sums[node] == max_sum: max_sum_set.add(node)\n",
    "        # If only one, then that's the next node... otherwise, choose the one with the highest min\n",
    "        if len(max_sum_set) <= 1:\n",
    "            next_node = max_sum_set.pop()\n",
    "        else:\n",
    "            for node in max_sum_set:\n",
    "                if mins[node] == max_sum_min:\n",
    "                    max_sum_node = node\n",
    "                    max_sum_min  = mins[node]\n",
    "            next_node = max_sum_node\n",
    "        found.add(next_node)\n",
    "    return found\n",
    "\n",
    "_node_colors_ = {}\n",
    "for _landmark_ in identifyLandmarks(_g_, 8): _node_colors_[_landmark_] = 'red'\n",
    "\n",
    "_pos_ = PolarsSpringLayout(_g_).results()\n",
    "rt.link(_df_, _relates_, _pos_, node_color=_node_colors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa43e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(1e10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d5ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
