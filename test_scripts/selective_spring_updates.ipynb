{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff931a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Tests the ability to use springs layout selectively on subgraphs\n",
    "# ... so, if you keep the placement of the nodes from an initial all-nodes layout\n",
    "# ... then you can update the placement of a subset of the nodes\n",
    "#\n",
    "# However... because that's not the only transform done to make this all work\n",
    "# (think about the treemap component placement), then there's always a scaling factor\n",
    "# involved... and that breaks the ability to apply the spring layout to a subset of \n",
    "# nodes...\n",
    "#\n",
    "\n",
    "import polars   as pl\n",
    "import networkx as nx\n",
    "import random\n",
    "import copy\n",
    "import rtsvg\n",
    "import linknode_graph_patterns\n",
    "from rtsvg.polars_spring_layout import PolarsSpringLayout\n",
    "rt   = rtsvg.RACETrack()\n",
    "'''\n",
    "g    = linknode_graph_patterns.LinkNodeGraphPatterns().__pattern_mesh__()\n",
    "_lu_ = {'fm':[], 'to':[]}\n",
    "for n in g.nodes():\n",
    "    for nbor in g.neighbors(n): _lu_['fm'].append(n), _lu_['to'].append(nbor)\n",
    "df        = pl.DataFrame(_lu_)\n",
    "_to_fix_  = {'node_0_5', 'node_7_4', 'node_8_6', 'node_5_8'}\n",
    "'''\n",
    "\n",
    "_base_network_ = '0'\n",
    "_base_dir_     = '../../data/stanford/facebook/'\n",
    "_layout_file_  = _base_dir_ + _base_network_ + '.layout.parquet'\n",
    "_edges_ = open(_base_dir_ + _base_network_ + '.edges', 'rt').read()\n",
    "_lu_ = {'fm':[], 'to':[]}\n",
    "for _edge_ in _edges_.split('\\n'):\n",
    "    if _edge_ == '': continue\n",
    "    _lu_['fm'].append(_edge_.split(' ')[0])\n",
    "    _lu_['to'].append(_edge_.split(' ')[1])\n",
    "df        = pl.DataFrame(_lu_)\n",
    "_relates_ = [('fm','to')]\n",
    "g         = rt.createNetworkXGraph(df, _relates_)\n",
    "_to_fix_  = {'110', '193', '201', '245', '259', '264', '61', '8', '91'}\n",
    "\n",
    "_relates_ = [('fm','to')]\n",
    "_colors_  = {n:'red' for n in _to_fix_}\n",
    "_psl_     = PolarsSpringLayout(g)\n",
    "pos       = _psl_.results()\n",
    "new_pos   = copy.deepcopy(pos)\n",
    "for n in _to_fix_: new_pos[n] = (new_pos[n][0] + 100*(random.random()-0.5), new_pos[n][1] + 100*(random.random()-0.5))\n",
    "new_new_pos = copy.deepcopy(new_pos)\n",
    "new_new_pos = PolarsSpringLayout(g, pos=new_new_pos, static_nodes=set(g.nodes()) - _to_fix_, normalize_coordinates=True).results()\n",
    "params      = {'w':384, 'h':384, 'node_color':_colors_}\n",
    "rt.tile([rt.link(df, _relates_, pos,         **params),\n",
    "         rt.link(df, _relates_, new_pos,     **params),\n",
    "         rt.link(df, _relates_, new_new_pos, **params)], spacer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acdfe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# So... maybe it's possible to rescale the graph based on the expected distances\n",
    "# ... in this case (mesh graph), all edges should be 1.0 ... so... how to take\n",
    "# ... a random set of positions & then rescale them?\n",
    "#\n",
    "_d_sum_, _d_samples_, _rms_sum_ = 0.0, 0, 0.0\n",
    "for n in g.nodes():\n",
    "    for nbor in g.neighbors(n):\n",
    "        _w_                  =  g[n][nbor]['weight']\n",
    "        _xy_n_, _xy_nbor_    =  pos[n], pos[nbor]\n",
    "        _d_                  =  rt.segmentLength((_xy_n_, _xy_nbor_))\n",
    "        _w_diff_             =  _w_ - _d_\n",
    "        _rms_sum_            += _w_diff_**2\n",
    "        _d_sum_, _d_samples_ = _d_sum_ + _d_, _d_samples_ + 1\n",
    "print(_d_sum_, _d_samples_, _d_sum_ / _d_samples_) # 359.24853900508407 288 1.2473907604343197 -- example from the mesh graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Grid Search Version of This Optimization\n",
    "#\n",
    "_lu_ = {'x_scale':[], 'y_scale':[], 'rms':[]}\n",
    "_inc_     = 0.05\n",
    "_x_scale_ = 0.5\n",
    "while _x_scale_ < 1.5:\n",
    "    _y_scale_ = 0.5\n",
    "    while _y_scale_ < 1.5:\n",
    "        _rms_ = _psl_.rootMeanSquareError(_x_scale_, _y_scale_)\n",
    "        _lu_['x_scale'].append(_x_scale_)\n",
    "        _lu_['y_scale'].append(_y_scale_)\n",
    "        _lu_['rms'].append(_rms_)\n",
    "        _y_scale_ += _inc_\n",
    "    _x_scale_ += _inc_\n",
    "\n",
    "rt.tile([rt.xy(pl.DataFrame(_lu_), x_field='x_scale', y_field='rms', line_groupby_field='y_scale', color_by='y_scale', dot_size=None),\n",
    "         rt.xy(pl.DataFrame(_lu_), x_field='y_scale', y_field='rms', line_groupby_field='x_scale', color_by='x_scale', dot_size=None)], spacer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40090ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ChatGPT Response (2025-09-10)\n",
    "# Prompt: \"Given three points (p0, p1, and p2), calculate the parabolic parameters a, b, and c.\"\n",
    "#\n",
    "import numpy as np\n",
    "def fit_parabola_numpy(points):\n",
    "    (x0,y0),(x1,y1),(x2,y2) = points\n",
    "    A = np.array([[x0*x0, x0, 1.0],\n",
    "                  [x1*x1, x1, 1.0],\n",
    "                  [x2*x2, x2, 1.0]], dtype=float)\n",
    "    y = np.array([y0,y1,y2], dtype=float)\n",
    "    a,b,c = np.linalg.solve(A,y)\n",
    "    return a,b,c\n",
    "\n",
    "# Calculate rms at intervals...\n",
    "_lu_ = {'x_scale':[], 'rms':[], 'group':[]}\n",
    "x_scale = 0.1\n",
    "while x_scale < 2.0:\n",
    "    rms = _psl_.rootMeanSquareError(x_scale, 1.0)\n",
    "    _lu_['x_scale'].append(x_scale), _lu_['rms'].append(rms), _lu_['group'].append('rms')\n",
    "    x_scale += 0.1\n",
    "\n",
    "# Find the approximate valley\n",
    "for i in range(1,len(_lu_['rms'])-1):\n",
    "    if _lu_['rms'][i] < _lu_['rms'][i-1] and _lu_['rms'][i] < _lu_['rms'][i+1]:\n",
    "        print(f'Found! ({i=})')\n",
    "        p0 = (_lu_['x_scale'][i-1], _lu_['rms'][i-1])\n",
    "        p1 = (_lu_['x_scale'][i],   _lu_['rms'][i])\n",
    "        p2 = (_lu_['x_scale'][i+1], _lu_['rms'][i+1])\n",
    "\n",
    "# Fit to a parabola\n",
    "_a_, _b_, _c_ = fit_parabola_numpy([p0, p1, p2]) \n",
    "_a_, _b_, _c_ = float(_a_), float(_b_), float(_c_)\n",
    "\n",
    "# Find the bottom of that ... i think (?) ... not tested yet\n",
    "if _a_ > 0.0: # opens upward...\n",
    "    x = -_b_ / 2 * _a_\n",
    "    y = _a_ * x**2 + _b_ * x + _c_ \n",
    "else: raise Exception('Parabola opens downward...')\n",
    "x,y\n",
    "\n",
    "# Produce the formula of the fitted parabola\n",
    "_lu2_ = {'x_scale':[], 'rms':[], 'group':[]}\n",
    "x_scale = 0.1\n",
    "while x_scale < 2.0:\n",
    "    _lu2_['x_scale'].append(x_scale), _lu2_['rms'].append(_a_ * x_scale**2 + _b_ * x_scale + _c_), _lu2_['group'].append('formula')\n",
    "    x_scale += 0.1\n",
    "_df_ = pl.concat([pl.DataFrame(_lu_), pl.DataFrame(_lu2_)])\n",
    "\n",
    "# Display what's found\n",
    "rt.tile([rt.xy(_df_, x_field='x_scale', y_field='rms',     w=768, h=512, line_groupby_field=\"group\", color_by='group')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e644d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
